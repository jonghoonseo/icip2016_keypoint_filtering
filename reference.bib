
@article{fisher_use_1936,
	title = {The Use of Multiple Measurements in Taxonomic Problems},
	volume = {7},
	copyright = {1936 Blackwell Publishing Ltd/University College London},
	issn = {2050-1439},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract},
	doi = {10.1111/j.1469-1809.1936.tb02137.x},
	abstract = {The articles published by the Annals of Eugenics (1925–1954) have been made available online as an historical archive intended for scholarly use. The work of eugenicists was often pervaded by prejudice against racial, ethnic and disabled groups. The online publication of this material for scholarly research purposes is not an endorsement of those views nor a promotion of eugenics in any way.},
	language = {en},
	number = {2},
	urldate = {2014-07-20},
	journal = {Annals of Eugenics},
	author = {Fisher, R. A.},
	month = sep,
	year = {1936},
	pages = {179--188},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\EUP9FW5K\\Fisher - 1936 - The Use of Multiple Measurements in Taxonomic Prob.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\PRHC8WDB\\full.html:text/html}
}

@article{chang_p-slam:_2007,
	title = {P-{SLAM}: Simultaneous Localization and Mapping With Environmental-Structure Prediction},
	volume = {23},
	issn = {1552-3098},
	shorttitle = {P-{SLAM}},
	doi = {10.1109/TRO.2007.892230},
	abstract = {Traditionally, simultaneous localization and mapping ({SLAM}) algorithms solve the localization and mapping problem in explored regions. This paper presents a prediction-based {SLAM} algorithm (called P-{SLAM}), which has an environmental-structure predictor to predict the structure inside an unexplored region (i.e., look-ahead mapping). The prediction process is based on the observation of the surroundings of an unexplored region and comparing it with the built map of explored regions. If a similar environment/structure is matched in the map of explored regions, a hypothesis is generated to indicate that a similar structure has been explored before. If the environment has repeated structures, the mobile robot can use the predicted structure as a virtual mapping, and decide whether or not to explore the unexplored region to save the exploration time. If the mobile robot decides to explore the unexplored region, a correct prediction can be used to speed up the {SLAM} process and build a more accurate map. We have also derived the Bayesian formulation of P-{SLAM} to show its compact recursive form for real-time operation. We have experimentally implemented the proposed P-{SLAM} on a Pioneer 3-{DX} mobile robot using a Rao-Blackwellized particle filter in real time. Computer simulations and experimental results validated the performance of the proposed P-{SLAM} and its effectiveness in indoor environments},
	number = {2},
	journal = {{IEEE} Transactions on Robotics},
	author = {Chang, H.J. and Lee, C. S G and Lu, Yung-Hsiang and Hu, Y.C.},
	month = apr,
	year = {2007},
	keywords = {Bayesian formulation, Bayesian methods, Bayes procedures, Computer Simulation, environmental-structure prediction, hypothesis generation, image matching, indoor environments, Instruction sets, map matching, mobile robots, Orbital robotics, Particle filters, Pioneer 3DX, Prediction algorithms, P-{SLAM}, Rao-Blackwellized particle filter, robot vision, search problems, simultaneous localization and mapping, simultaneous localization and mapping ({SLAM}), {SLAM} (robots), unexplored region mapping, virtual mapping},
	pages = {281--293},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\V3599BBS\\articleDetails.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\H75T68W8\\Chang et al. - 2007 - P-SLAM Simultaneous Localization and Mapping With.pdf:application/pdf}
}

@incollection{frahm_building_2010,
	series = {Lecture Notes in Computer Science},
	title = {Building Rome on a Cloudless Day},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-15560-4, 978-3-642-15561-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-15561-1_27},
	abstract = {This paper introduces an approach for dense 3D reconstruction from unregistered Internet-scale photo collections with about 3 million images within the span of a day on a single {PC} (“cloudless”). Our method advances image clustering, stereo, stereo fusion and structure from motion to achieve high computational performance. We leverage geometric and appearance constraints to obtain a highly parallel implementation on modern graphics processors and multi-core architectures. This leads to two orders of magnitude higher performance on an order of magnitude larger dataset than competing state-of-the-art approaches.},
	language = {en},
	number = {6314},
	urldate = {2014-08-22},
	booktitle = {Computer Vision – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Frahm, Jan-Michael and Fite-Georgel, Pierre and Gallup, David and Johnson, Tim and Raguram, Rahul and Wu, Changchang and Jen, Yi-Hung and Dunn, Enrique and Clipp, Brian and Lazebnik, Svetlana and Pollefeys, Marc},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	month = jan,
	year = {2010},
	keywords = {Algorithm Analysis and Problem Complexity, Biometrics, Computer graphics, Computer Imaging, Vision, Pattern Recognition and Graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {368--381},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\VD7HREX6\\Frahm et al. - 2010 - Building Rome on a Cloudless Day.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\8PQFWTHD\\978-3-642-15561-1_27.html:text/html}
}

@inproceedings{snavely_photo_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGGRAPH} '06},
	title = {Photo Tourism: Exploring Photo Collections in 3D},
	isbn = {1-59593-364-6},
	shorttitle = {Photo Tourism},
	url = {http://doi.acm.org/10.1145/1179352.1141964},
	doi = {10.1145/1179352.1141964},
	abstract = {We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.},
	urldate = {2014-08-22},
	booktitle = {{ACM} {SIGGRAPH} 2006 Papers},
	publisher = {{ACM}},
	author = {Snavely, Noah and Seitz, Steven M. and Szeliski, Richard},
	year = {2006},
	keywords = {image-based modeling, image-based rendering, photo browsing, structure from motion},
	pages = {835--846},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\XBWKW9C3\\Snavely et al. - 2006 - Photo Tourism Exploring Photo Collections in 3D.pdf:application/pdf}
}

@inproceedings{brown_recognising_2003,
	title = {Recognising panoramas},
	doi = {10.1109/ICCV.2003.1238630},
	abstract = {The problem considered in this paper is the fully automatic construction of panoramas. Fundamentally, this problem requires recognition, as we need to know which parts of the panorama join up. Previous approaches have used human input or restrictions on the image sequence for the matching step. In this work we use object recognition techniques based on invariant local features to select matching images, and a probabilistic model for verification. Because of this our method is insensitive to the ordering, orientation, scale and illumination of the images. It is also insensitive to 'noise' images which are not part of the panorama at all, that is, it recognises panoramas. This suggests a useful application for photographers: the system takes as input the images on an entire flash card or film, recognises images that form part of a panorama, and stitches them with no user input whatsoever.},
	booktitle = {Ninth {IEEE} International Conference on Computer Vision, 2003. Proceedings},
	author = {Brown, M. and Lowe, D.G.},
	month = oct,
	year = {2003},
	keywords = {automatic panorama construction, automatic panorama stitching, camera matrix, Computer vision, digital cameras, feature extraction, feature matching, image illumination, image matching, image ordering, image orientation, image reconstruction, image scale, image sequence, image sequences, invariant local features, Leverberg-Marquardt algorithm, local intensity values, multiband blending, nonlinear least squares problem, normalised cross-correlation, object recognition, object recognition techniques, panorama recognition, panoramic image geometry, panoramic image mosaicing, photographers, scale invariant feature transform},
	pages = {1218--1225 vol.2},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\4HSIEHMX\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\PVZS35RU\\Brown 그리고 Lowe - 2003 - Recognising panoramas.pdf:application/pdf}
}

@inproceedings{nister_scalable_2006,
	title = {Scalable Recognition with a Vocabulary Tree},
	volume = {2},
	doi = {10.1109/CVPR.2006.264},
	abstract = {A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes {CD}-covers from a database of 40000 images of popular music {CD}’s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.},
	booktitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Nister, D. and Stewenius, H.},
	year = {2006},
	keywords = {Computer vision, Frequency, Image databases, Image recognition, indexing, Quantization, Robustness, Spatial databases, Visualization, vocabulary},
	pages = {2161--2168},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\NJW7BHF5\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\6JW3JQKZ\\Nister 그리고 Stewenius - 2006 - Scalable Recognition with a Vocabulary Tree.pdf:application/pdf}
}

@misc{_seoul_2014,
	title = {Seoul Travel Guide},
	url = {http://www.visitseoul.net/en/guide/ebook.do?_method=list&m=0004011019003&p=04},
	journal = {Visit Seoul: The Official Travel Guide to Seoul},
	month = feb,
	year = {2014},
	file = {Seoul Travel Guide.pdf:E\:\\Papers\\Zotero Repository\\storage\\PPG4M4MH\\Seoul Travel Guide.pdf:application/pdf}
}

@article{yu_novel_2012,
	title = {A Novel Algorithm for View and Illumination Invariant Image Matching},
	volume = {21},
	issn = {1057-7149},
	doi = {10.1109/TIP.2011.2160271},
	abstract = {The challenges in local-feature-based image matching are variations of view and illumination. Many methods have been recently proposed to address these problems by using invariant feature detectors and distinctive descriptors. However, the matching performance is still unstable and inaccurate, particularly when large variation in view or illumination occurs. In this paper, we propose a view and illumination invariant image-matching method. We iteratively estimate the relationship of the relative view and illumination of the images, transform the view of one image to the other, and normalize their illumination for accurate matching. Our method does not aim to increase the invariance of the detector but to improve the accuracy, stability, and reliability of the matching results. The performance of matching is significantly improved and is not affected by the changes of view and illumination in a valid range. The proposed method would fail when the initial view and illumination method fails, which gives us a new sight to evaluate the traditional detectors. We propose two novel indicators for detector evaluation, namely, valid angle and valid illumination, which reflect the maximum allowable change in view and illumination, respectively. Extensive experimental results show that our method improves the traditional detector significantly, even in large variations, and the two indicators are much more distinctive.},
	number = {1},
	journal = {{IEEE} Transactions on Image Processing},
	author = {Yu, Yinan and Huang, Kaiqi and Chen, Wei and Tan, Tieniu},
	month = jan,
	year = {2012},
	keywords = {Algorithms, Detectors, distinctive descriptors, Estimation, Feature detector evaluation, feature extraction, Histograms, illumination variations, Image Enhancement, Image Interpretation, Computer-Assisted, image matching, invariant feature detectors, invariant image matching, iterative estimation, iterative methods, Lighting, local feature, Pattern Recognition, Automated, reliability, Reproducibility of Results, Sensitivity and Specificity, Stability, Transforms, valid angle ({VA}), valid illumination ({VI})},
	pages = {229--240},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\QDBRP97X\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\DNEIS29R\\Yu et al. - 2012 - A Novel Algorithm for View and Illumination Invari.pdf:application/pdf}
}

@article{thirion_image_1998,
	title = {Image matching as a diffusion process: an analogy with Maxwell's demons},
	volume = {2},
	issn = {1361-8415},
	shorttitle = {Image matching as a diffusion process},
	url = {http://www.sciencedirect.com/science/article/pii/S1361841598800224},
	doi = {10.1016/S1361-8415(98)80022-4},
	abstract = {In this paper, we present the concept of diffusing models to perform image-to-image matching. Having two images to match, the main idea is to consider the objects boundaries in one image as semi-permeable membranes and to let the other image, considered as a deformable grid model, diffuse through these interfaces, by the action of effectors situated within the membranes. We illustrate this concept by an analogy with Maxwell's demons. We show that this concept relates to more traditional ones, based on attraction, with an intermediate step being optical flow techniques. We use the concept of diffusing models to derive three different non-rigid matching algorithms, one using all the intensity levels in the static image, one using only contour points, and a last one operating on already segmented images. Finally, we present results with synthesized deformations and real medical images, with applications to heart motion tracking and three-dimensional inter-patients matching.},
	number = {3},
	urldate = {2014-09-01},
	journal = {Medical Image Analysis},
	author = {Thirion, J. -P.},
	month = sep,
	year = {1998},
	keywords = {deformable model, elastic matching, image sequence analysis, inter-patient registration, non-rigid matching},
	pages = {243--260},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\TT4A2UMI\\Thirion - 1998 - Image matching as a diffusion process an analogy .pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\VF5UVSJ9\\S1361841598800224.html:text/html}
}

@inproceedings{tollmar_picture_2007,
	address = {New York, {NY}, {USA}},
	series = {{MobileHCI} '07},
	title = {A Picture is Worth a Thousand Keywords: Exploring Mobile Image-based Web Search},
	isbn = {978-1-59593-862-6},
	shorttitle = {A Picture is Worth a Thousand Keywords},
	url = {http://doi.acm.org/10.1145/1377999.1378048},
	doi = {10.1145/1377999.1378048},
	abstract = {Images of objects as queries is a new approach to search for information on the web. Image-based information retrieval goes beyond only matching images, as information in other modalities also can be extracted from data collections using image search. We have developed a new system that uses images to search for web-based information. This paper has a particular focus towards exploring user's experience of general mobile image-based Web searches to find what issues and phenomena it contains. This was achieved in a multi-part study by creating and letting respondents test prototypes of mobile image-based search systems and collecting data using interviews, observations, video-observations, and questionnaires. We observed that searching for information only based on visual similarity and without any assistance is sometimes difficult, especially on mobile devices with limited interaction bandwidth. Most of our subjects preferred a search tool that guides the users through the search result based on contextual information, compared to presenting the search result as a plain ranked list.},
	urldate = {2014-09-01},
	booktitle = {Proceedings of the 9th International Conference on Human Computer Interaction with Mobile Devices and Services},
	publisher = {{ACM}},
	author = {Tollmar, Konrad and Möller, Ted and Nilsved, Björn},
	year = {2007},
	keywords = {content-based image retrieval, mobile information retrieval, mobile interface, object recognition},
	pages = {421--428},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\4XFA6CMX\\Tollmar et al. - 2007 - A Picture is Worth a Thousand Keywords Exploring .pdf:application/pdf}
}

@inproceedings{chen_tree_2009,
	title = {Tree Histogram Coding for Mobile Image Matching},
	doi = {10.1109/DCC.2009.33},
	abstract = {For mobile image matching applications, a mobile device captures a query image, extracts descriptive features, and transmits these features wirelessly to a server. The server recognizes the query image by comparing the extracted features to its database and returns information associated with the recognition result. For slow links, query feature compression is crucial for low-latency retrieval. Previous image retrieval systems transmit compressed feature descriptors, which is well suited for pairwise image matching. For fast retrieval from large databases, however, scalable vocabulary trees are commonly employed. In this paper, we propose a rate-efficient codec designed for tree-based retrieval. By encoding a tree histogram, our codec can achieve a more than 5times rate reduction compared to sending compressed feature descriptors. By discarding the order amongst a list of features, histogram coding requires 1.5times lower rate than sending a tree node index for every feature. A statistical analysis is performed to study how the entropy of encoded symbols varies with tree depth and the number of features.},
	booktitle = {Data Compression Conference, 2009. {DCC} '09.},
	author = {Chen, D.M. and Tsai, S.S. and Chandrasekhar, V. and Takacs, G. and Singh, J. and Girod, B.},
	month = mar,
	year = {2009},
	keywords = {Codecs, Databases, data compression, Data mining, descriptive feature extraction, feature extraction, histogram coding, Histograms, image coding, Image databases, image matching, Image recognition, Image retrieval, image retrieval systems, low-latency retrieval, mobile device, mobile image matching, query image, rate-efficient codec, robust features, scalable vocabulary tree, Spatial databases, tree histogram coding, vector quantization, visual databases},
	pages = {143--152},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\MR59R7QU\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\4TX2I9EV\\Chen et al. - 2009 - Tree Histogram Coding for Mobile Image Matching.pdf:application/pdf}
}

@inproceedings{yeh_searching_2004,
	title = {Searching the Web with mobile images for location recognition},
	volume = {2},
	doi = {10.1109/CVPR.2004.1315147},
	abstract = {We describe an approach to recognizing location from mobile devices using image-based Web search. We demonstrate the usefulness of common image search metrics applied on images captured with a camera-equipped mobile device to find matching images on the World Wide Web or other general-purpose databases. Searching the entire Web can be computationally overwhelming, so we devise a hybrid image-and-keyword searching technique. First, image-search is performed over images and links to their source Web pages in a database that indexes only a small fraction of the Web. Then, relevant keywords on these Web pages are automatically identified and submitted to an existing text-based search engine (e.g. Google) that indexes a much larger portion of the Web. Finally, the resulting image set is filtered to retain images close to the original query. It is thus possible to efficiently search hundreds of millions of images that are not only textually related but also visually relevant. We demonstrate our approach on an application allowing users to browse Web pages matching the image of a nearby location.},
	booktitle = {Proceedings of the 2004 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 2004. {CVPR} 2004},
	author = {Yeh, T. and Tollmar, K. and Darrell, T.},
	month = jun,
	year = {2004},
	keywords = {camera-equipped mobile device, Content based retrieval, hybrid image-and-keyword searching technique, image-based Web search, Image databases, image matching, Image recognition, Image retrieval, image search metrics, Indexes, Internet, location recognition, mobile images, Personal digital assistants, query formulation, search engines, text-based search engine, very large databases, video databases, Web pages, Web search, Web sites, World Wide Web},
	pages = {II--76--II--81 Vol.2},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\F5PXDDNF\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\4Z2MC7J6\\Yeh et al. - 2004 - Searching the Web with mobile images for location .pdf:application/pdf}
}

@inproceedings{zhang_image_2006,
	title = {Image Based Localization in Urban Environments},
	doi = {10.1109/3DPVT.2006.80},
	abstract = {In this paper we present a prototype system for image based localization in urban environments. Given a database of views of city street scenes tagged by {GPS} locations, the system computes the {GPS} location of a novel query view. We first use a wide-baseline matching technique based on {SIFT} features to select the closest views in the database. Often due to a large change of viewpoint and presence of repetitive structures, a large percentage of matches ({\textgreater} 50\%) are not correct correspondences. The subsequent motion estimation between the query view and the reference view, is then handled by a novel and efficient robust estimation technique capable of dealing with large percentage of outliers. This stage is also accompanied by a model selection step among the fundamental matrix and the homography. Once the motion between the closest reference views is estimated, the location of the query view is then obtained by triangulation of translation directions. Approximate solutions for cases when triangulation cannot be obtained reliably are also described. The presented system is tested on the dataset used in {ICCV} 2005 Computer Vision Contest and is shown to have higher accuracy than previous reported results.},
	booktitle = {Third International Symposium on 3D Data Processing, Visualization, and Transmission},
	author = {Zhang, Wei and Kosecka, J.},
	month = jun,
	year = {2006},
	keywords = {Cities and towns, city street scenes, Computer vision, Global Positioning System, {GPS} locations, homography, image based localization, Image databases, image matching, image motion analysis, Layout, motion estimation, Prototypes, robust estimation technique, Robustness, {SIFT} features, Spatial databases, System testing, urban environments, wide-baseline matching technique},
	pages = {33--40},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\V3RWN7KQ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\QB45B4WQ\\Zhang 그리고 Kosecka - 2006 - Image Based Localization in Urban Environments.pdf:application/pdf}
}

@inproceedings{henrysson_umar:_2004,
	address = {New York, {NY}, {USA}},
	series = {{MUM} '04},
	title = {{UMAR}: Ubiquitous Mobile Augmented Reality},
	isbn = {1-58113-981-0},
	shorttitle = {{UMAR}},
	url = {http://doi.acm.org/10.1145/1052380.1052387},
	doi = {10.1145/1052380.1052387},
	abstract = {In this paper we discuss the prospects of using marker based Augmented Reality for context aware applications on mobile phones. We also present the {UMAR}, a conceptual framework for developing Ubiquitous Mobile Augmented Reality applications which consists of research areas identified as relevant for successfully bridging the physical world and the digital domain using Mobile Augmented Reality. A step towards this we have successfully ported the {ARToolkit} to consumer mobile phones running on the Symbian platform and present results around this. We also present three sample applications based on {UMAR} and future case study work planned.},
	urldate = {2014-09-01},
	booktitle = {Proceedings of the 3rd International Conference on Mobile and Ubiquitous Multimedia},
	publisher = {{ACM}},
	author = {Henrysson, Anders and Ollila, Mark},
	year = {2004},
	keywords = {augmented reality, pervasive computing},
	pages = {41--45},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\SPGCUF4S\\Henrysson 그리고 Ollila - 2004 - UMAR Ubiquitous Mobile Augmented Reality.pdf:application/pdf}
}

@inproceedings{takacs_outdoors_2008,
	address = {New York, {NY}, {USA}},
	series = {{MIR} '08},
	title = {Outdoors Augmented Reality on Mobile Phone Using Loxel-based Visual Feature Organization},
	isbn = {978-1-60558-312-9},
	url = {http://doi.acm.org/10.1145/1460096.1460165},
	doi = {10.1145/1460096.1460165},
	abstract = {We have built an outdoors augmented reality system for mobile phones that matches camera-phone images against a large database of location-tagged images using a robust image retrieval algorithm. We avoid network latency by implementing the algorithm on the phone and deliver excellent performance by adapting a state-of-the-art image retrieval algorithm based on robust local descriptors. Matching is performed against a database of highly relevant features, which is continuously updated to reflect changes in the environment. We achieve fast updates and scalability by pruning of irrelevant features based on proximity to the user. By compressing and incrementally updating the features stored on the phone we make the system amenable to low-bandwidth wireless connections. We demonstrate system robustness on a dataset of location-tagged images and show a smart-phone implementation that achieves a high image matching rate while operating in near real-time.},
	urldate = {2014-09-01},
	booktitle = {Proceedings of the 1st {ACM} International Conference on Multimedia Information Retrieval},
	publisher = {{ACM}},
	author = {Takacs, Gabriel and Chandrasekhar, Vijay and Gelfand, Natasha and Xiong, Yingen and Chen, Wei-Chao and Bismpigiannis, Thanos and Grzeszczuk, Radek and Pulli, Kari and Girod, Bernd},
	year = {2008},
	keywords = {augmented reality, geo-referenced photos, image search, local search, mobile computing, photo collections},
	pages = {427--434},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\UZZ56KJR\\Takacs et al. - 2008 - Outdoors Augmented Reality on Mobile Phone Using L.pdf:application/pdf}
}

@inproceedings{wagner_pose_2008,
	address = {Washington, {DC}, {USA}},
	series = {{ISMAR} '08},
	title = {Pose Tracking from Natural Features on Mobile Phones},
	isbn = {978-1-4244-2840-3},
	url = {http://dx.doi.org/10.1109/ISMAR.2008.4637338},
	doi = {10.1109/ISMAR.2008.4637338},
	abstract = {In this paper we present two techniques for natural feature tracking in real-time on mobile phones. We achieve interactive frame rates of up to 20Hz for natural feature tracking from textured planar targets on current-generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely {SIFT} and Ferns. While {SIFT} is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. We present evaluations on robustness and performance on various devices and finally discuss their appropriateness for Augmented Reality applications.},
	urldate = {2014-09-01},
	booktitle = {Proceedings of the 7th {IEEE}/{ACM} International Symposium on Mixed and Augmented Reality},
	publisher = {{IEEE} Computer Society},
	author = {Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, Dieter},
	year = {2008},
	pages = {125--134},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\GF8AMS66\\Wagner et al. - 2008 - Pose Tracking from Natural Features on Mobile Phon.pdf:application/pdf}
}

@inproceedings{chen_streaming_2009,
	title = {Streaming mobile augmented reality on mobile phones},
	doi = {10.1109/ISMAR.2009.5336472},
	abstract = {Continuous recognition and tracking of objects in live video captured on a mobile device enables real-time user interaction. We demonstrate a streaming mobile augmented reality system with 1 second latency. User interest is automatically inferred from camera movements, so the user never has to press a button. Our system is used to identify and track book and {CD} covers in real time on a phone's viewfinder. Efficient motion estimation is performed at 30 frames per second on a phone, while fast search through a database of 20,000 images is performed on a server.},
	booktitle = {8th {IEEE} International Symposium on Mixed and Augmented Reality, 2009. {ISMAR} 2009},
	author = {Chen, D.M. and Tsai, S.S. and Vedantham, R. and Grzeszczuk, R. and Girod, B.},
	month = oct,
	year = {2009},
	keywords = {augmented reality, book cover identification, book cover tracking, Books, camera movement, Cameras, {CD} cover tracking, continuous object recognition, continuous object tracking, Delay, Image databases, Layout, live video, mobile computing, mobile handsets, mobile phone, motion estimation, multimedia systems, object recognition, phone viewfinder, Real time systems, real-time user interaction, Streaming media, streaming mobile augmented reality, video signal processing, video streaming},
	pages = {181--182},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\9RJRIAD9\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\777JCGN8\\Chen et al. - 2009 - Streaming mobile augmented reality on mobile phone.pdf:application/pdf}
}

@article{se_vision-based_2005,
	title = {Vision-based global localization and mapping for mobile robots},
	volume = {21},
	issn = {1552-3098},
	doi = {10.1109/TRO.2004.839228},
	abstract = {We have previously developed a mobile robot system which uses scale-invariant visual landmarks to localize and simultaneously build three-dimensional (3-D) maps of unmodified environments. In this paper, we examine global localization, where the robot localizes itself globally, without any prior location estimate. This is achieved by matching distinctive visual landmarks in the current frame to a database map. A Hough transform approach and a {RANSAC} approach for global localization are compared, showing that {RANSAC} is much more efficient for matching specific features, but much worse for matching nonspecific features. Moreover, robust global localization can be achieved by matching a small submap of the local region built from multiple frames. This submap alignment algorithm for global localization can be applied to map building, which can be regarded as alignment of multiple 3-D submaps. A global minimization procedure is carried out using the loop closure constraint to avoid the effects of slippage and drift accumulation. Landmark uncertainty is taken into account in the submap alignment and the global minimization process. Experiments show that global localization can be achieved accurately using the scale-invariant landmarks. Our approach of pairwise submap alignment with backward correction in a consistent manner produces a better global 3-D map.},
	number = {3},
	journal = {{IEEE} Transactions on Robotics},
	author = {Se, S. and Lowe, D.G. and Little, J.J.},
	month = jun,
	year = {2005},
	keywords = {backward correction, Buildings, Computer vision, database map, Global localization, global minimization procedure, Hough transform, Hough transforms, Intelligent robots, Intelligent systems, landmark uncertainty, localization robustness, map building, minimisation, mobile robots, pairwise submap alignment algorithm, path planning, place recognition, {RANSAC} approach, robot vision, Robustness, scale-invariant visual landmarks, simultaneous localization and mapping, Spatial databases, three-dimensional maps, Uncertainty, vision-based global localization, visual databases, visual landmarks},
	pages = {364--375},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\8R3JGI9N\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\B4RSJP6Z\\Se et al. - 2005 - Vision-based global localization and mapping for m.pdf:application/pdf}
}

@article{lowe_distinctive_2004,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1023/B%3AVISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	language = {en},
	number = {2},
	urldate = {2014-09-01},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computer Imaging, Graphics and Computer Vision, image matching, Image Processing, invariant features, object recognition, scale invariance},
	pages = {91--110},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\94WW3KV8\\Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\XVU3U3NT\\BVISI.0000029664.99615.html:text/html}
}

@inproceedings{korman_fast-match:_2013,
	title = {{FasT}-Match: Fast Affine Template Matching},
	shorttitle = {{FasT}-Match},
	doi = {10.1109/CVPR.2013.302},
	abstract = {Fast-Match is a fast algorithm for approximate template matching under 2D affine transformations that minimizes the Sum-of-Absolute-Differences ({SAD}) error measure. There is a huge number of transformations to consider but we prove that they can be sampled using a density that depends on the smoothness of the image. For each potential transformation, we approximate the {SAD} error using a sub linear algorithm that randomly examines only a small number of pixels. We further accelerate the algorithm using a branch-and-bound scheme. As images are known to be piecewise smooth, the result is a practical affine template matching algorithm with approximation guarantees, that takes a few seconds to run on a standard machine. We perform several experiments on three different datasets, and report very good results. To the best of our knowledge, this is the first template matching algorithm which is guaranteed to handle arbitrary 2D affine transformations.},
	booktitle = {2013 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Korman, S. and Reichman, D. and Tsur, G. and Avidan, S.},
	month = jun,
	year = {2013},
	keywords = {2D affine transformations, Accuracy, affine transforms, Approximation algorithms, Approximation methods, branch-and-bound scheme, Computer vision, fast affine template matching, {FasT}-match, feature extraction, Gray-scale, image matching, image smoothness, sublinear algorithm, sum-of-absolute-differences error measure, tree searching},
	pages = {2331--2338},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\AURV437C\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\FPBTB6C4\\Korman et al. - 2013 - FasT-Match Fast Affine Template Matching.pdf:application/pdf}
}

@article{mehtre_color_1995,
	title = {Color matching for image retrieval},
	volume = {16},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/016786559400096L},
	doi = {10.1016/0167-8655(94)00096-L},
	abstract = {Color is an important attribute for image matching and retrieval. We present two new color matching methods, the “Reference Color Table Method” and a “Distance Method”, for image retrieval. Both these methods and an existing method “Histogram Intersection” were implemented and tested for a database size of 170 color images. To compare the efficacy of each method, a figure of merit, called “Efficiency of Retrieval”, is defined. The results show that both the new methods perform better than the existing method, and that the Reference Color Table Method gives the best results.},
	number = {3},
	urldate = {2014-12-11},
	journal = {Pattern Recognition Letters},
	author = {Mehtre, Babu M. and Kankanhalli, Mohan S. and Desai Narasimhalu, A. and Chang Man, Guo},
	month = mar,
	year = {1995},
	keywords = {Color, Distance, matching, Measure, Retrieval},
	pages = {325--331},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\T85QKWVG\\Mehtre et al. - 1995 - Color matching for image retrieval.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\GXQQK75Q\\016786559400096L.html:text/html}
}

@article{kankanhalli_cluster-based_1996,
	title = {Cluster-based color matching for image retrieval},
	volume = {29},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/0031320395000976},
	doi = {10.1016/0031-3203(95)00097-6},
	abstract = {Color is an important attribute for image matching and retrieval. We present a new method fo color matching based on a clustering algorithm in 3-D color space. We define a new color feature to characterize the color information and a distance measure to compute the color similarity of images. We have implemented this technique and tested it for a database of approximately 170 images. The test results shoe that the ‘Efficiency of Retrieval’ of this new method is very high.},
	number = {4},
	urldate = {2014-12-11},
	journal = {Pattern Recognition},
	author = {Kankanhalli, Mohan S. and Mehtre, Babu M. and Wu, Ran Kang},
	month = apr,
	year = {1996},
	keywords = {Clustering, Color, matching, Measure, Retrieval},
	pages = {701--708},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\RM2TUC4F\\Kankanhalli et al. - 1996 - Cluster-based color matching for image retrieval.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\AWCJ6NNX\\0031320395000976.html:text/html}
}

@incollection{le_improving_2013,
	series = {Lecture Notes in Computer Science},
	title = {Improving Histogram-Based Image Registration in Video Sequences through Warping},
	copyright = {©2013 Springer International Publishing Switzerland},
	isbn = {978-3-319-03730-1, 978-3-319-03731-8},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-03731-8_25},
	abstract = {This paper presents two computationally efficient dynamic-time warping algorithms for image registration in video sequence through histogram based image segmentation. The key idea is to warp the histogram in an input frame to create an approximation of a reference frame. Any histogram based thresholding method can then be applied to create consistent regions in both the input and the approximated reference frames. Experiments of the proposed algorithm are used to demonstrate that more consistent matches can found after thresholding.},
	language = {en},
	number = {8294},
	urldate = {2014-12-11},
	booktitle = {Advances in Multimedia Information Processing – {PCM} 2013},
	publisher = {Springer International Publishing},
	author = {Le, Xuesong and Gonzalez, Ruben},
	editor = {Huet, Benoit and Ngo, Chong-Wah and Tang, Jinhui and Zhou, Zhi-Hua and Hauptmann, Alexander G. and Yan, Shuicheng},
	month = jan,
	year = {2013},
	keywords = {Computer Imaging, Vision, Pattern Recognition and Graphics, consistent regions, Data Mining and Knowledge Discovery, dynamic-time warping, histogram-based, Information Systems Applications (incl. Internet), Multimedia Information Systems, Software Engineering, User Interfaces and Human Computer Interaction},
	pages = {269--280},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\B3DHDTT5\\Le 그리고 Gonzalez - 2013 - Improving Histogram-Based Image Registration in Vi.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\884QMZIJ\\978-3-319-03731-8_25.html:text/html}
}

@article{goncalves_hairis:_2011,
	title = {{HAIRIS}: A Method for Automatic Image Registration Through Histogram-Based Image Segmentation},
	volume = {20},
	issn = {1057-7149},
	shorttitle = {{HAIRIS}},
	doi = {10.1109/TIP.2010.2076298},
	abstract = {Automatic image registration is still an actual challenge in several fields. Although several methods for automatic image registration have been proposed in the last few years, it is still far from a broad use in several applications, such as in remote sensing. In this paper, a method for automatic image registration through histogram-based image segmentation ({HAIRIS}) is proposed. This new approach mainly consists in combining several segmentations of the pair of images to be registered, according to a relaxation parameter on the histogram modes delineation (which itself is a new approach), followed by a consistent characterization of the extracted objects-through the objects area, ratio between the axis of the adjust ellipse, perimeter and fractal dimension-and a robust statistical based procedure for objects matching. The application of the proposed methodology is illustrated to simulated rotation and translation. The first dataset consists in a photograph and a rotated and shifted version of the same photograph, with different levels of added noise. It was also applied to a pair of satellite images with different spectral content and simulated translation, and to real remote sensing examples comprising different viewing angles, different acquisition dates and different sensors. An accuracy below 1° for rotation and at the subpixel level for translation were obtained, for the most part of the considered situations. {HAIRIS} allows for the registration of pairs of images (multitemporal and multisensor) with differences in rotation and translation, with small differences in the spectral content, leading to a subpixel accuracy.},
	number = {3},
	journal = {{IEEE} Transactions on Image Processing},
	author = {Goncalves, H. and Goncalves, J.A. and Corte-Real, L.},
	month = mar,
	year = {2011},
	keywords = {acquisition dates, added noise, automatic image registration, Degradation, fractal dimension, {HAIRIS}, Histogram, histogram-based image segmentation, histogram modes delineation, Histograms, image registration, image segmentation, matching, Noise, object area, object detection, object matching, photograph, relaxation parameter, Remote sensing, rotation, satellite images, Satellites, sensors, spectral content, statistical analysis, statistical procedure, subpixel accuracy, translation, viewing angles, Wiener filtering},
	pages = {776--789},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\ZD7FB6AW\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\SGNE79VK\\Goncalves et al. - 2011 - HAIRIS A Method for Automatic Image Registration .pdf:application/pdf}
}

@inproceedings{gruber_optimization_2010,
	title = {Optimization of Target Objects for Natural Feature Tracking},
	doi = {10.1109/ICPR.2010.880},
	abstract = {This paper investigates possible physical alterations of tracking targets to obtain improved 6DoF pose detection for a camera observing the known targets. We explore the influence of several texture characteristics on the pose detection, by simulating a large number of different target objects and camera poses. Based on statistical observations, we rank the importance of characteristics such as texturedness and feature distribution for a specific implementation of a 6DoF tracking technique. These findings allow informed modification strategies for improving the tracking target objects themselves, in the common case of man-made targets, as for example used in advertising. This fundamentally differs from and complements the traditional approach of leaving the targets unchanged while trying to optimize the tracking algorithms and parameters.},
	booktitle = {2010 20th International Conference on Pattern Recognition ({ICPR})},
	author = {Gruber, L. and Zollmann, S. and Wagner, Daniel and Schmalstieg, D. and Hollerer, T.},
	month = aug,
	year = {2010},
	keywords = {6DoF tracking, camera pose, Cameras, Correlation, feature distribution, feature extraction, image texture, Lighting, man-made target, natural feature tracking, object detection, object target tracking, optical tracking, optimization, pose detection, pose estimation, Robots, simulation, statistical analysis, statistical observation, target tracking, tracking target optimization},
	pages = {3607--3610},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\CGJ5GR4G\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\Z7BI2REJ\\Gruber et al. - 2010 - Optimization of Target Objects for Natural Feature.pdf:application/pdf}
}

@inproceedings{gruber_evaluating_2009,
	title = {Evaluating the trackability of natural feature-point sets},
	doi = {10.1109/ISMAR.2009.5336469},
	abstract = {In this work we present a novel idea of evaluating natural feature-point based tracking targets. Our main objective is to evaluate the inherent characteristics of natural feature-point sets with respect to vision-based pose estimation algorithms. Our work attempts to break new ground by concentrating on evaluating complete tracking targets, rather than evaluating tracking methods or single features. This allows deriving indications on how to improve the trackability of natural feature point sets.},
	booktitle = {8th {IEEE} International Symposium on Mixed and Augmented Reality, 2009. {ISMAR} 2009},
	author = {Gruber, L. and Zollmann, S. and Wagner, Daniel and Schmalstieg, D.},
	month = oct,
	year = {2009},
	keywords = {Algorithm design and analysis, augmented reality, Computational modeling, Computer vision, Image Processing, Karhunen-Loeve transforms, natural feature-point sets, Natural Feature Tracking Target Design, object detection, Pipelines, pose estimation, Robustness, Runtime, target tracking, trackability, Tracking Simulation, vision-based pose estimation algorithms},
	pages = {189--190},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\WTCRUPD5\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\7CT8B7TW\\Gruber et al. - 2009 - Evaluating the trackability of natural feature-poi.pdf:application/pdf}
}

@inproceedings{oerlemans_interest_2008,
	address = {New York, {NY}, {USA}},
	series = {{MIR} '08},
	title = {Interest Points Based on Maximization of Distinctiveness},
	isbn = {978-1-60558-312-9},
	url = {http://doi.acm.org/10.1145/1460096.1460130},
	doi = {10.1145/1460096.1460130},
	abstract = {Interest or salient points are typically meaningful points within an image which can be used for a wide variety of image understanding tasks. In this paper we present a novel algorithm for detecting interest points within images. The new technique is based on finding the locations in an image which exhibit local distinctiveness. We evaluate our algorithm on the Corel stock photography test set in the context of content based image retrieval from large databases and provide quantitative comparisons to the well known {SIFT} interest point and Harris corner detectors as a benchmark.},
	urldate = {2014-10-20},
	booktitle = {Proceedings of the 1st {ACM} International Conference on Multimedia Information Retrieval},
	publisher = {{ACM}},
	author = {Oerlemans, Ard and Lew, Michael S.},
	year = {2008},
	keywords = {Image retrieval, interest points},
	pages = {202--207},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\QQ2U5VD6\\Oerlemans 그리고 Lew - 2008 - Interest Points Based on Maximization of Distincti.pdf:application/pdf}
}

@inproceedings{brown_multi-image_2005,
	address = {Washington, {DC}, {USA}},
	series = {{CVPR} '05},
	title = {Multi-Image Matching Using Multi-Scale Oriented Patches},
	isbn = {0-7695-2372-2},
	url = {http://dx.doi.org/10.1109/CVPR.2005.235},
	doi = {10.1109/CVPR.2005.235},
	abstract = {This paper describes a novel multi-view matching framework based on a new type of invariant feature. Our features are located at Harris corners in discrete scale-space and oriented using a blurred local gradient. This defines a rotationally invariant frame in which we sample a feature descriptor, which consists of an 8 × 8 patch of bias/gain normalised intensity values. The density of features in the image is controlled using a novel adaptive non-maximal suppression algorithm, which gives a better spatial distribution of features than previous approaches. Matching is achieved using a fast nearest neighbour algorithm that indexes features based on their low frequency Haar wavelet coefficients. We also introduce a novel outlier rejection procedure that verifies a pairwise feature match based on a background distribution of incorrect feature matches. Feature matches are refined using {RANSAC} and used in an automatic 2D panorama stitcher that has been extensively tested on hundreds of sample inputs.},
	urldate = {2014-10-20},
	booktitle = {Proceedings of the 2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05) - Volume 1 - Volume 01},
	publisher = {{IEEE} Computer Society},
	author = {Brown, Matthew and Szeliski, Richard and Winder, Simon},
	year = {2005},
	pages = {510--517},
	file = {Brown et al. - 2005 - Multi-Image Matching Using Multi-Scale Oriented Pa.pdf:E\:\\Papers\\Zotero Repository\\storage\\JWZXQE49\\Brown et al. - 2005 - Multi-Image Matching Using Multi-Scale Oriented Pa.pdf:application/pdf}
}

@inproceedings{gauglitz_efficiently_2011,
	title = {Efficiently selecting spatially distributed keypoints for visual tracking},
	doi = {10.1109/ICIP.2011.6115832},
	abstract = {We describe an algorithm dubbed Suppression via Disk Covering ({SDC}) to efficiently select a set of strong, spatially distributed key-points, and we show that selecting keypoint in this way significantly improves visual tracking. We also describe two efficient implementation schemes for the popular Adaptive Non-Maximal Suppression algorithm, and show empirically that {SDC} is significantly faster while providing the same improvements with respect to tracking robustness. In our particular application, using {SDC} to filter the output of an inexpensive (but, by itself, less reliable) keypoint detector ({FAST}) results in higher tracking robustness at significantly lower total cost than using a computationally more expensive detector.},
	booktitle = {2011 18th {IEEE} International Conference on Image Processing ({ICIP})},
	author = {Gauglitz, S. and Foschini, L. and Turk, M. and Hollerer, T.},
	month = sep,
	year = {2011},
	keywords = {adaptive nonmaximal suppression algorithm, Conferences, Data structures, Detectors, disk covering, dubbed suppression, filtering theory, inexpensive keypoint detector, object tracking, Robustness, robustness tracking, Runtime, spatially distributed keypoint, target tracking, Visual tracking},
	pages = {1869--1872},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\RVWIMD5D\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\CGRV2NQB\\Gauglitz et al. - 2011 - Efficiently selecting spatially distributed keypoi.pdf:application/pdf}
}

@inproceedings{shi_good_1994,
	title = {Good features to track},
	doi = {10.1109/CVPR.1994.323794},
	abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments},
	booktitle = {, 1994 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 1994. Proceedings {CVPR} '94},
	author = {Shi, J. and Tomasi, C.},
	month = jun,
	year = {1994},
	keywords = {affine image transformations, Computer vision, disocclusions, feature-based, feature extraction, feature monitoring, feature selection, Machine vision, Newton-Raphson style search methods, occlusions, Performance, tracker, tracking, vision system},
	pages = {593--600},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\I5CPJ9IT\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\DATACD6A\\Shi 그리고 Tomasi - 1994 - Good features to track.pdf:application/pdf}
}

@inproceedings{knapek_selecting_2000,
	title = {Selecting promising landmarks},
	volume = {4},
	doi = {10.1109/ROBOT.2000.845319},
	abstract = {Many approaches to visual servoing and mobile robot navigation are based on tracking feature points or landmarks in images, but not all features points are equally effective as landmarks. Here we develop methods for selecting within an image those landmarks which are both perceptually salient and visually distinctive, and consequently are readily recognized in a second image acquired from a different viewpoint. Empirically, we characterize the performance of the recognition method and then demonstrate that the selection process does in fact choose the landmarks which are more likely to be recognized},
	booktitle = {{IEEE} International Conference on Robotics and Automation, 2000. Proceedings. {ICRA} '00},
	author = {Knapek, M. and Oropeza, R.S. and Kriegman, D.J.},
	year = {2000},
	keywords = {Character recognition, computerised navigation, Computer science, feature extraction, Fires, Image recognition, image sequences, Layout, Microcomputers, mobile robot, mobile robots, Navigation, object recognition, optical tracking, path planning, promising landmark selection, robot vision, tracking, Visual servoing},
	pages = {3771--3777 vol.4},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\2S72FAAZ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\HP2W3IIW\\Knapek et al. - 2000 - Selecting promising landmarks.pdf:application/pdf}
}

@article{mikolajczyk_performance_2005,
	title = {A performance evaluation of local descriptors},
	volume = {27},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2005.188},
	abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector [Mikolajczyk, K and Schmid, C, 2004]. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [Belongie, S, et al., April 2002], steerable filters [Freeman, W and Adelson, E, Setp. 1991], {PCA}-{SIFT} [Ke, Y and Sukthankar, R, 2004], differential invariants [Koenderink, J and van Doorn, A, 1987], spin images [Lazebnik, S, et al., 2003], {SIFT} [Lowe, D. G., 1999], complex filters [Schaffalitzky, F and Zisserman, A, 2002], moment invariants [Van Gool, L, et al., 1996], and cross-correlation for different types of interest regions. We also propose an extension of the {SIFT} descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the {SIFT}-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.},
	number = {10},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Mikolajczyk, K. and Schmid, C.},
	month = oct,
	year = {2005},
	keywords = {Algorithms, Artificial Intelligence, complex filters, Computer Simulation, correlation methods, cross-correlation, Data Interpretation, Statistical, Detectors, filtering theory, Filters, image classification, Image databases, Image Enhancement, Image Interpretation, Computer-Assisted, image matching, Image recognition, Image retrieval, image transformations, Index Terms- Local descriptors, Information retrieval, Information Storage and Retrieval, interest points, interest regions, invariance, Layout, local descriptors, matching, Models, Statistical, moment invariants, Pattern Recognition, Automated, performance evaluation, recognition., Robustness, Software, Software Validation, Spatial databases, spin images, steerable filters},
	pages = {1615--1630},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\78BEBMX2\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\G5C2PUXX\\Mikolajczyk 그리고 Schmid - 2005 - A performance evaluation of local descriptors.pdf:application/pdf}
}

@inproceedings{leutenegger_brisk:_2011,
	title = {{BRISK}: Binary Robust invariant scalable keypoints},
	shorttitle = {{BRISK}},
	doi = {10.1109/ICCV.2011.6126542},
	abstract = {Effective and efficient generation of keypoints from an image is a well-studied problem in the literature and forms the basis of numerous Computer Vision applications. Established leaders in the field are the {SIFT} and {SURF} algorithms which exhibit great performance under a variety of image transformations, with {SURF} in particular considered as the most computationally efficient amongst the high-performance methods to date. In this paper we propose {BRISK}, a novel method for keypoint detection, description and matching. A comprehensive evaluation on benchmark datasets reveals {BRISK}'s adaptive, high quality performance as in state-of-the-art algorithms, albeit at a dramatically lower computational cost (an order of magnitude faster than {SURF} in cases). The key to speed lies in the application of a novel scale-space {FAST}-based detector in combination with the assembly of a bit-string descriptor from intensity comparisons retrieved by dedicated sampling of each keypoint neighborhood.},
	booktitle = {2011 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {Leutenegger, S. and Chli, M. and Siegwart, R.Y.},
	year = {2011},
	keywords = {binary robust invariant scalable keypoints, bit-string descriptor, Boats, Brightness, {BRISK} method, Complexity theory, Computer vision, computer vision application, Detectors, feature extraction, image matching, image transformation, Kernel, keypoint description, keypoint detection, keypoint generation, keypoint matching, Robustness, scale-space {FAST}-based detector, {SIFT} algorithm, {SURF} algorithm, Transforms},
	pages = {2548--2555},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\WNP96HME\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\8ISADMNZ\\Leutenegger et al. - 2011 - BRISK Binary Robust invariant scalable keypoints.pdf:application/pdf}
}

@incollection{calonder_brief:_2010,
	series = {Lecture Notes in Computer Science},
	title = {{BRIEF}: Binary Robust Independent Elementary Features},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-15560-4, 978-3-642-15561-1},
	shorttitle = {{BRIEF}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-15561-1_56},
	abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call {BRIEF}.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L 2 norm as is usually done. As a result, {BRIEF} is very fast both to build and to match. We compare it against {SURF} and U-{SURF} on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either.},
	number = {6314},
	urldate = {2014-01-27},
	booktitle = {Computer Vision – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	month = jan,
	year = {2010},
	keywords = {Algorithm Analysis and Problem Complexity, Biometrics, Computer graphics, Computer Imaging, Vision, Pattern Recognition and Graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {778--792},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\E6D2N6K6\\Calonder et al. - 2010 - BRIEF Binary Robust Independent Elementary Featur.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\KD9EQ9KM\\978-3-642-15561-1_56.html:text/html}
}

@inproceedings{alahi_freak:_2012,
	title = {{FREAK}: Fast Retina Keypoint},
	shorttitle = {{FREAK}},
	doi = {10.1109/CVPR.2012.6247715},
	abstract = {A large number of vision applications rely on matching keypoints across images. The last decade featured an arms-race towards faster and more robust keypoints and association algorithms: Scale Invariant Feature Transform ({SIFT})[17], Speed-up Robust Feature ({SURF})[4], and more recently Binary Robust Invariant Scalable Keypoints ({BRISK})[I6] to name a few. These days, the deployment of vision algorithms on smart phones and embedded devices with low memory and computation complexity has even upped the ante: the goal is to make descriptors faster to compute, more compact while remaining robust to scale, rotation and noise. To best address the current requirements, we propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Keypoint ({FREAK}). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. Our experiments show that {FREAKs} are in general faster to compute with lower memory load and also more robust than {SIFT}, {SURF} or {BRISK}. They are thus competitive alternatives to existing keypoints in particular for embedded applications.},
	booktitle = {2012 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Alahi, A. and Ortiz, R. and Vandergheynst, P.},
	month = jun,
	year = {2012},
	keywords = {association algorithm, binary robust invariant scalable keypoint, binary string, computational complexity, computation complexity, Detectors, embedded application, embedded device, eye, fast retina keypoint, Humans, human visual system, image matching, Kernel, keypoint descriptor, keypoint matching, Noise, Retina, Robustness, scale invariant feature transform, {SIFT}, smart phone, smart phones, speed-up robust feature, {SURF}, Transforms, Vectors, vision application},
	pages = {510--517},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\H7PUBCTC\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\WVGU7KAI\\Alahi et al. - 2012 - FREAK Fast Retina Keypoint.pdf:application/pdf}
}

@incollection{heinly_comparative_2012,
	series = {Lecture Notes in Computer Science},
	title = {Comparative Evaluation of Binary Features},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-33708-6, 978-3-642-33709-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33709-3_54},
	abstract = {Performance evaluation of salient features has a long-standing tradition in computer vision. In this paper, we fill the gap of evaluation for the recent wave of binary feature descriptors, which aim to provide robustness while achieving high computational efficiency. We use established metrics to embed our assessment into the body of existing evaluations, allowing us to provide a novel taxonomy unifying both traditional and novel binary features. Moreover, we analyze the performance of different detector and descriptor pairings, which are often used in practice but have been infrequently analyzed. Additionally, we complement existing datasets with novel data testing for illumination change, pure camera rotation, pure scale change, and the variety present in photo-collections. Our performance analysis clearly demonstrates the power of the new class of features. To benefit the community, we also provide a website for the automatic testing of new description methods using our provided metrics and datasets ( www.cs.unc.edu/feature-evaluation ).},
	urldate = {2014-05-20},
	booktitle = {Computer Vision – {ECCV} 2012},
	publisher = {Springer Berlin Heidelberg},
	author = {Heinly, Jared and Dunn, Enrique and Frahm, Jan-Michael},
	editor = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
	month = jan,
	year = {2012},
	keywords = {Algorithm Analysis and Problem Complexity, binary features, Biometrics, comparison, Computer graphics, Computer Imaging, Vision, Pattern Recognition and Graphics, evaluation, Image Processing and Computer Vision, Pattern Recognition},
	pages = {759--773},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\MZBIIHHX\\Heinly et al. - 2012 - Comparative Evaluation of Binary Features.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\G6U8R3HB\\978-3-642-33709-3_54.html:text/html}
}

@article{lowe_distinctive_2004-1,
	title = {Distinctive Image Features from Scale-Invariant Keypoints},
	volume = {60},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1023/B%3AVISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	language = {en},
	number = {2},
	urldate = {2014-02-03},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computer Imaging, Graphics and Computer Vision, image matching, Image Processing, invariant features, object recognition, scale invariance},
	pages = {91--110},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\XT6Z9DX3\\Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\DPZRFMC4\\BVISI.0000029664.99615.html:text/html}
}

@inproceedings{ke_pca-sift:_2004,
	title = {{PCA}-{SIFT}: a more distinctive representation for local image descriptors},
	volume = {2},
	shorttitle = {{PCA}-{SIFT}},
	doi = {10.1109/CVPR.2004.1315206},
	abstract = {Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid (June 2003) recently evaluated a variety of approaches and identified the {SIFT} [D. G. Lowe, 1999] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by {SIFT}. Like {SIFT}, our descriptors encode the salient aspects of the image gradient in the feature point's neighborhood; however, instead of using {SIFT}'s smoothed weighted histograms, we apply principal components analysis ({PCA}) to the normalized gradient patch. Our experiments demonstrate that the {PCA}-based local descriptors are more distinctive, more robust to image deformations, and more compact than the standard {SIFT} representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.},
	booktitle = {Proceedings of the 2004 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 2004. {CVPR} 2004},
	author = {Ke, Yan and Sukthankar, R.},
	month = jun,
	year = {2004},
	keywords = {Computer science, Computer vision, feature extraction, Filters, Histograms, image deformations, image gradient, image registration, image representation, Image retrieval, image retrieval application, local feature detection, local image descriptor, object detection, object recognition, object recognition algorithms, principal component analysis, principal components analysis, Robustness},
	pages = {II--506--II--513 Vol.2},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\CNNHUIET\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\F52SPK2V\\Ke 그리고 Sukthankar - 2004 - PCA-SIFT a more distinctive representation for lo.pdf:application/pdf}
}

@article{bay_speeded-up_2008,
	series = {Similarity Matching in Computer Vision and Multimedia},
	title = {Speeded-Up Robust Features ({SURF})},
	volume = {110},
	issn = {1077-3142},
	url = {http://www.sciencedirect.com/science/article/pii/S1077314207001555},
	doi = {10.1016/j.cviu.2007.09.014},
	abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined {SURF} (Speeded-Up Robust Features). {SURF} approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.

This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps.

The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important parameters. We conclude the article with {SURF}’s application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline {SURF}’s usefulness in a broad range of topics in computer vision.},
	number = {3},
	urldate = {2014-06-27},
	journal = {Computer Vision and Image Understanding},
	author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Van Gool, Luc},
	month = jun,
	year = {2008},
	keywords = {camera calibration, Feature description, interest points, Local features, object recognition},
	pages = {346--359},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\3EX7IX9B\\Bay et al. - 2008 - Speeded-Up Robust Features (SURF).pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\WFSXVUQX\\S1077314207001555.html:text/html}
}

@inproceedings{rublee_orb:_2011,
	title = {{ORB}: An efficient alternative to {SIFT} or {SURF}},
	shorttitle = {{ORB}},
	doi = {10.1109/ICCV.2011.6126544},
	abstract = {Feature matching is at the base of many computer vision problems, such as object recognition or structure from motion. Current methods rely on costly descriptors for detection and matching. In this paper, we propose a very fast binary descriptor based on {BRIEF}, called {ORB}, which is rotation invariant and resistant to noise. We demonstrate through experiments how {ORB} is at two orders of magnitude faster than {SIFT}, while performing as well in many situations. The efficiency is tested on several real-world applications, including object detection and patch-tracking on a smart phone.},
	booktitle = {2011 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {Rublee, E. and Rabaud, V. and Konolige, K. and Bradski, G.},
	month = nov,
	year = {2011},
	keywords = {binary descriptor, Boats, {BRIEF}, Computer vision, feature matching, image matching, noise resistance, object detection, object recognition, {ORB}, patch-tracking, {SIFT}, smart phone, {SURF}, tracking, Transforms},
	pages = {2564--2571},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\B5I3HCB2\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\23XEDX82\\Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF.pdf:application/pdf}
}

@article{schmid_local_1997,
	title = {Local Greyvalue Invariants for Image Retrieval},
	volume = {19},
	abstract = {This paper addresses the problem of retrieving images from large image databases. The method is based on local greyvalue invariants which are computed at automatically detected interest points. A voting algorithm and semi-local constraints make retrieval possible. Indexing allows for efficient retrieval from a database of more than 1000 images. Experimental results show correct retrieval in the case of partial visibility, similarity transformations, extraneous features, and small perspective deformations.  Keywords--- image retrieval, image indexing, greylevel invariants, matching I. Introduction  This paper addresses the problem of matching an image to a large set of images. The query image is a new (partial) image of an object imaged in the database. The image may be taken from a different viewing angle (made precise below).  A. Existing recognition methods  Existing approaches in the literature are of two types : those that use geometric features of an object ; and those that rely o...},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Schmid, Cordelia and Mohr, Roger},
	year = {1997},
	pages = {530--535},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\6V7DEG3E\\Schmid 그리고 Mohr - 1997 - Local Greyvalue Invariants for Image Retrieval.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\4R9VTBKX\\summary.html:text/html}
}

@article{freeman_design_1991,
	title = {The Design and Use of Steerable Filters},
	volume = {13},
	issn = {0162-8828},
	doi = {10.1109/34.93808},
	number = {9},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Freeman, W. T. and Adelson, E. H.},
	year = {1991},
	pages = {891--906},
	file = {IEEE Computer Snapshot:E\:\\Papers\\Zotero Repository\\storage\\44NU4A6V\\i0891.html:text/html}
}

@inproceedings{carneiro_multi-scale_2003,
	title = {Multi-scale phase-based local features},
	volume = {1},
	doi = {10.1109/CVPR.2003.1211426},
	abstract = {Local feature methods suitable for image feature based object recognition and for the estimation of motion and structure are composed of two steps, namely the 'where' and 'what' steps. The 'where' step (e.g., interest point detector) must select image points that are robustly localizable under common image deformations and whose neighborhoods are relatively informative. The 'what' step (e.g., local feature extractor) then provides a representation of the image neighborhood that is semi-invariant to image deformations, but distinctive enough to provide model identification. We present a quantitative evaluation of both the 'where' and the 'what' steps for three recent local feature methods: a) phase-based local features (Carneiro and Jepson, 2002), b) differential invariants (Schmid and Mohr, 1997), and c) the scale invariant feature transform ({SIFT}) (Lowe, 1999). Moreover, in order to make the phase-based approach more comparable to the other two approaches, we also introduce a new form of multi-scale interest point detector to be used for its 'where' step. The results show that the phase-based local features lead to better performance than the other two approaches when dealing with common illumination changes, 2D rotation, and sub-pixel translation. On the other hand, the phase-based local features are somewhat more sensitive to scale and large shear changes than the other two methods. Finally, we demonstrate the viability of the phase-based local feature in a simple object recognition system.},
	booktitle = {2003 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings},
	author = {Carneiro, G. and Jepson, A.D.},
	month = jun,
	year = {2003},
	keywords = {2D rotation, Computer science, Data mining, Deformable models, Detectors, differential invariant, edge detection, feature extraction, feature matching, illumination change, image deformation, image feature, image matching, image neighborhood representation, image point selection, image representation, image texture, interest point detection, model identification, motion estimation, multiscale local feature, object recognition, object recognition system, phase-based local feature, Phase detection, Position measurement, principal component analysis, quantitative evaluation, Robustness, scale invariant feature transform, semiinvariant representation, {SIFT}, structure estimation, subpixel translation},
	pages = {I--736--I--743 vol.1},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\NUHN7HQN\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\BGD43T5D\\Carneiro 그리고 Jepson - 2003 - Multi-scale phase-based local features.pdf:application/pdf}
}

@inproceedings{yu_fully_2009,
	title = {A fully affine invariant image comparison method},
	doi = {10.1109/ICASSP.2009.4959904},
	abstract = {A fully affine invariant image comparison method, Affine-{SIFT} ({ASIFT}) is introduced. While {SIFT} is fully invariant with respect to only four parameters namely zoom, rotation and translation, the new method treats the two left over parameters : the angles defining the camera axis orientation. Against any prognosis, simulating all views depending on these two parameters is feasible. The method permits to reliably identify features that have undergone very large affine distortions measured by a new parameter, the transition tilt. State-of-the-art methods hardly exceed transition tilts of 2 ({SIFT}), 2.5 (Harris-Affine and Hessian-Affine) and 10 ({MSER}). {ASIFT} can handle transition tilts up 36 and higher (see Fig. 1).},
	booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing, 2009. {ICASSP} 2009},
	author = {Yu, Guoshen and Morel, J.-M.},
	month = apr,
	year = {2009},
	keywords = {affine invariance, affine invariant image comparison method, affine normalization, affine transforms, Cameras, Costs, Deformable models, Detectors, Distortion measurement, Ear, Equations, Image edge detection, image matching, Optical distortion, scale invariance, {SIFT}},
	pages = {1597--1600},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\KVNCEP7U\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IHPTACRK\\Yu 그리고 Morel - 2009 - A fully affine invariant image comparison method.pdf:application/pdf}
}

@inproceedings{wu_evaluation_2013,
	address = {New York, {NY}, {USA}},
	series = {{MM} '13},
	title = {Evaluation of Salient Point Methods},
	isbn = {978-1-4503-2404-5},
	url = {http://doi.acm.org/10.1145/2502081.2502179},
	doi = {10.1145/2502081.2502179},
	abstract = {Processing visual content in images and videos is a challenging task associated with the development of modern computer vision. Because salient point approaches can represent distinctive and affine invariant points in images, many approaches have been proposed over the past decade. Each method has particular advantages and limitations and may be appropriate in different contexts. In this paper we evaluate the performance of a wide set of salient point detectors and descriptors. We begin by comparing diverse salient point algorithms ({SIFT}, {SURF}, {BRIEF}, {ORB}, {FREAK}, {BRISK}, {STAR}, {GFTT} and {FAST}) with regard to repeatability, recall and precision and then move to accuracy and stability in real-time video tracking.},
	urldate = {2014-05-27},
	booktitle = {Proceedings of the 21st {ACM} International Conference on Multimedia},
	publisher = {{ACM}},
	author = {Wu, Song and Lew, Michael},
	year = {2013},
	keywords = {evaluation, salient point methods, video tracking},
	pages = {685--688},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\495PQ29G\\Wu 그리고 Lew - 2013 - Evaluation of Salient Point Methods.pdf:application/pdf}
}

@article{schmid_evaluation_2000,
	title = {Evaluation of Interest Point Detectors},
	volume = {37},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1023/A%3A1008199403446},
	doi = {10.1023/A:1008199403446},
	abstract = {Many different low-level feature detectors exist and it is widely agreed that the evaluation of detectors is important. In this paper we introduce two evaluation criteria for interest points' repeatability rate and information content. Repeatability rate evaluates the geometric stability under different transformations. Information content measures the distinctiveness of features. Different interest point detectors are compared using these two criteria. We determine which detector gives the best results and show that it satisfies the criteria well.},
	language = {en},
	number = {2},
	urldate = {2014-05-28},
	journal = {International Journal of Computer Vision},
	author = {Schmid, Cordelia and Mohr, Roger and Bauckhage, Christian},
	month = jun,
	year = {2000},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, comparison of detectors, Computer Imaging, Graphics and Computer Vision, Image Processing, information content, interest points, quantitative evaluation, repeatability},
	pages = {151--172},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\FJ8PJ5S8\\Schmid et al. - 2000 - Evaluation of Interest Point Detectors.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\ZKQZHABK\\A1008199403446.html:text/html}
}

@inproceedings{miksik_evaluation_2012,
	title = {Evaluation of local detectors and descriptors for fast feature matching},
	abstract = {Local feature detectors and descriptors are widely used in many computer vision applications and various methods have been proposed during the past decade. There have been a number of evaluations focused on various aspects of local features, matching accuracy in particular, however there has been no comparisons considering the accuracy and speed trade-offs of recent extractors such as {BRIEF}, {BRISK}, {ORB}, {MRRID}, {MROGH} and {LIOP}. This paper provides a performance evaluation of recent feature detectors and compares their matching precision and speed in randomized kd-trees setup as well as an evaluation of binary descriptors with efficient computation of Hamming distance.},
	booktitle = {2012 21st International Conference on Pattern Recognition ({ICPR})},
	author = {Miksik, O. and Mikolajczyk, K.},
	month = nov,
	year = {2012},
	keywords = {Accuracy, Approximation methods, Artificial neural networks, binary descriptor evaluation, {BRIEF} extractor, {BRISK} extractor, Computer vision, computer vision applications, Databases, Detectors, feature detector performance evaluation, feature extraction, feature matching, Hamming distance, image matching, {LIOP} extractor, local feature descriptor evaluation, local feature detector evaluation, matching accuracy, matching precision, matching speed, {MROGH} extractor, {MRRID} extractor, object detection, {ORB} extractor, randomized kd-trees, random processes, tree data structures},
	pages = {2681--2684},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\W82FI2HD\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\T98TE6UM\\Miksik 그리고 Mikolajczyk - 2012 - Evaluation of local detectors and descriptors for .pdf:application/pdf}
}

@book{szeliski_computer_????,
	title = {Computer Vision - Algorithms and Applications},
	isbn = {978-1-84882-935-0},
	url = {http://www.springer.com/computer/image+processing/book/978-1-84882-934-3},
	abstract = {Humans perceive the three-dimensional structure of the world with apparent ease. However, despite all of the recent advances in computer vision research, the dream of having a computer interpret an image at the same level as a ...},
	urldate = {2014-06-06},
	author = {Szeliski, Richard},
	keywords = {Computer Vision - Algorithms and Applications, Image Processing and Computer Vision},
	file = {Computer Vision\: Algorithms and Applications:E\:\\Papers\\Zotero Repository\\storage\\8FN4W5BA\\Book.html:text/html;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\6M5UJVT4\\978-1-84882-934-3.html:text/html;Szeliski - Computer Vision - Algorithms and Applications.pdf:E\:\\Papers\\Zotero Repository\\storage\\JUAS369V\\Szeliski - Computer Vision - Algorithms and Applications.pdf:application/pdf}
}

@inproceedings{bekele_evaluation_2013,
	title = {Evaluation of binary keypoint descriptors},
	doi = {10.1109/ICIP.2013.6738753},
	abstract = {In this paper an evaluation of state-of-the-art binary keypoint descriptors, namely {BRIEF}, {ORB}, {BRISK} and {FREAK}, is presented. In contrast to previous evaluations we used the Stanford Mobile Visual Search ({SMVS}) data set because binary descriptors are mainly used in mobile applications. This large data set does provide a lot of characteristic transformations for mobile devices, but no ground truth data. The often used Oxford data set is used only for validation purposes. We use ratio-test and {RANSAC} ({RANdom} {SAmple} Consensus) for evaluation and present results for accuracy, precision and average number of best matches as performance metrics. The validity of the results is also checked by evaluating these binary keypoint descriptors on Oxford data set. The obtained results show that {BRISK} is the keypoint descriptor which gives highest percentage of precision and largest number of best matches among all the binary descriptors. Next to {BRISK} is {FREAK}, which offers comparably good result.},
	booktitle = {2013 20th {IEEE} International Conference on Image Processing ({ICIP})},
	author = {Bekele, D. and Teutsch, M. and Schuchert, T.},
	month = sep,
	year = {2013},
	keywords = {binary descriptors, binary keypoint descriptors, {BRIEF}, {BRISK}, characteristic transformations, evaluation, feature extraction, {FREAK}, image matching, Image retrieval, invariance, matching, mobile applications, mobile computing, mobile devices, mobile feature tracking, object tracking, {ORB}, Oxford data set, random processes, random sammple consensus, {RANSAC}, ratio-test, recognition, {SMVS} data set, Stanford mobile visual search, statistical testing},
	pages = {3652--3656},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\CZ946PU6\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\B8H84SSK\\Bekele et al. - 2013 - Evaluation of binary keypoint descriptors.pdf:application/pdf}
}

@article{gauglitz_evaluation_2011,
	title = {Evaluation of Interest Point Detectors and Feature Descriptors for Visual Tracking},
	volume = {94},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1007/s11263-011-0431-5},
	doi = {10.1007/s11263-011-0431-5},
	abstract = {Applications for real-time visual tracking can be found in many areas, including visual odometry and augmented reality. Interest point detection and feature description form the basis of feature-based tracking, and a variety of algorithms for these tasks have been proposed. In this work, we present (1) a carefully designed dataset of video sequences of planar textures with ground truth, which includes various geometric changes, lighting conditions, and levels of motion blur, and which may serve as a testbed for a variety of tracking-related problems, and (2) a comprehensive quantitative evaluation of detector-descriptor-based visual camera tracking based on this testbed. We evaluate the impact of individual algorithm parameters, compare algorithms for both detection and description in isolation, as well as all detector-descriptor combinations as a tracking solution. In contrast to existing evaluations, which aim at different tasks such as object recognition and have limited validity for visual tracking, our evaluation is geared towards this application in all relevant factors (performance measures, testbed, candidate algorithms). To our knowledge, this is the first work that comprehensively compares these algorithms in this context, and in particular, on video streams.},
	language = {en},
	number = {3},
	urldate = {2014-09-06},
	journal = {International Journal of Computer Vision},
	author = {Gauglitz, Steffen and Höllerer, Tobias and Turk, Matthew},
	month = sep,
	year = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Imaging, Vision, Pattern Recognition and Graphics, Dataset, evaluation, Feature descriptors, Image Processing and Computer Vision, Interest point detectors, Pattern Recognition, Visual tracking},
	pages = {335--360},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\EJK6J845\\Gauglitz et al. - 2011 - Evaluation of Interest Point Detectors and Feature.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\57XV3IJE\\s11263-011-0431-5.html:text/html}
}

@article{sivic_efficient_2009,
	title = {Efficient Visual Search of Videos Cast as Text Retrieval},
	volume = {31},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2008.111},
	abstract = {We describe an approach to object retrieval which searches for and localizes all the occurrences of an object in a video, given a query image of the object. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject those that are unstable. Efficient retrieval is achieved by employing methods from statistical text retrieval, including inverted file systems, and text and document frequency weightings. This requires a visual analogy of a word which is provided here by vector quantizing the region descriptors. The final ranking also depends on the spatial layout of the regions. The result is that retrieval is immediate, returning a ranked list of shots in the manner of Google. We report results for object retrieval on the full length feature films 'Groundhog Day', 'Casablanca' and 'Run Lola Run', including searches from within the movie and specified by external images downloaded from the Internet. We investigate retrieval performance with respect to different quantizations of region descriptors and compare the performance of several ranking measures.},
	number = {4},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Sivic, J. and Zisserman, A.},
	month = apr,
	year = {2009},
	keywords = {document frequency weightings, frame matching, Image/video retrieval, inverted file systems, object recognition, object retrieval, query image, query processing, region descriptors, statistical text retrieval, text analysis, vector quantizing, video retrieval, visual search},
	pages = {591--606},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\9VA57I78\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\JPSNDGV6\\Sivic 그리고 Zisserman - 2009 - Efficient Visual Search of Videos Cast as Text Ret.pdf:application/pdf}
}

@article{fischler_random_1981,
	title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
	volume = {24},
	issn = {0001-0782},
	shorttitle = {Random Sample Consensus},
	url = {http://doi.acm.org/10.1145/358669.358692},
	doi = {10.1145/358669.358692},
	abstract = {A new paradigm, Random Sample Consensus ({RANSAC}), for fitting a model to experimental data is introduced. {RANSAC} is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of {RANSAC} to the Location Determination Problem ({LDP}): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a {RANSAC} requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the {LDP} under difficult viewing},
	number = {6},
	urldate = {2014-11-24},
	journal = {Commun. {ACM}},
	author = {Fischler, Martin A. and Bolles, Robert C.},
	month = jun,
	year = {1981},
	keywords = {automated cartography, camera calibration, image matching, location determination, model fitting, scene analysis},
	pages = {381--395},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\KKAW8SCF\\Fischler 그리고 Bolles - 1981 - Random Sample Consensus A Paradigm for Model Fitt.pdf:application/pdf}
}

@incollection{mair_adaptive_2010,
	series = {Lecture Notes in Computer Science},
	title = {Adaptive and Generic Corner Detection Based on the Accelerated Segment Test},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-15551-2, 978-3-642-15552-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-15552-9_14},
	abstract = {The efficient detection of interesting features is a crucial step for various tasks in Computer Vision. Corners are favored cues due to their two dimensional constraint and fast algorithms to detect them. Recently, a novel corner detection approach, {FAST}, has been presented which outperforms previous algorithms in both computational performance and repeatability. We will show how the accelerated segment test, which underlies {FAST}, can be significantly improved by making it more generic while increasing its performance.We do so by finding the optimal decision tree in an extended configuration space, and demonstrating how specialized trees can be combined to yield an adaptive and generic accelerated segment test. The resulting method provides high performance for arbitrary environments and so unlike {FAST} does not have to be adapted to a specific scene structure. We will also discuss how different test patterns affect the corner response of the accelerated segment test.},
	number = {6312},
	urldate = {2014-05-20},
	booktitle = {Computer Vision – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Mair, Elmar and Hager, Gregory D. and Burschka, Darius and Suppa, Michael and Hirzinger, Gerhard},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	month = jan,
	year = {2010},
	keywords = {Algorithm Analysis and Problem Complexity, Biometrics, Computer graphics, Computer Imaging, Vision, Pattern Recognition and Graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {183--196},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\8EGI7WS8\\Mair et al. - 2010 - Adaptive and Generic Corner Detection Based on the.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\4Q2MPTPX\\10.html:text/html}
}

@incollection{rosten_machine_2006,
	series = {Lecture Notes in Computer Science},
	title = {Machine Learning for High-Speed Corner Detection},
	copyright = {©2006 Springer Berlin Heidelberg},
	isbn = {978-3-540-33832-1, 978-3-540-33833-8},
	url = {http://link.springer.com/chapter/10.1007/11744023_34},
	abstract = {Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as {SIFT} ({DoG}), Harris and {SUSAN} are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live {PAL} video using less than 7\% of the available processing time. By comparison neither the Harris detector (120\%) nor the detection stage of {SIFT} (300\%) can operate at full frame rate. Clearly a high-speed detector is of limited use if the features produced are unsuitable for downstream processing. In particular, the same scene viewed from two different positions should yield features which correspond to the same real-world 3D locations [1]. Hence the second contribution of this paper is a comparison corner detectors based on this criterion applied to 3D scenes. This comparison supports a number of claims made elsewhere concerning existing corner detectors. Further, contrary to our initial expectations, we show that despite being principally constructed for speed, our detector significantly outperforms existing feature detectors according to this criterion.},
	language = {en},
	number = {3951},
	urldate = {2014-07-03},
	booktitle = {Computer Vision – {ECCV} 2006},
	publisher = {Springer Berlin Heidelberg},
	author = {Rosten, Edward and Drummond, Tom},
	editor = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
	month = jan,
	year = {2006},
	keywords = {Artificial Intelligence (incl. Robotics), Computer graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {430--443},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\EH9PW2IG\\Rosten 그리고 Drummond - 2006 - Machine Learning for High-Speed Corner Detection.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\6AMFJZNQ\\11744023_34.html:text/html}
}

@inproceedings{harris_combined_1988,
	title = {A combined corner and edge detector},
	abstract = {Consistency of image edge filtering is of prime importance for 3D interpretation of image sequences using feature tracking algorithms. To cater for image regions containing texture and isolated features, a combined corner and edge detector based on the local auto-correlation function is utilised, and it is shown to perform with good consistency on natural imagery.},
	booktitle = {Proceedings of Fourth Alvey Vision Conference},
	author = {Harris, Chris and Stephens, Mike},
	year = {1988},
	pages = {147--151},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\E42XTDXQ\\Harris 그리고 Stephens - 1988 - A combined corner and edge detector.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\KMIZ7ZMB\\summary.html:text/html}
}

@inproceedings{shi_good_1994-1,
	title = {Good features to track},
	doi = {10.1109/CVPR.1994.323794},
	abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments},
	booktitle = {, 1994 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 1994. Proceedings {CVPR} '94},
	author = {Shi, J. and Tomasi, C.},
	month = jun,
	year = {1994},
	keywords = {affine image transformations, Computer vision, disocclusions, feature-based, feature extraction, feature monitoring, feature selection, Machine vision, Newton-Raphson style search methods, occlusions, Performance, tracker, tracking, vision system},
	pages = {593--600},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\48DZQWJD\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\ZPDNXGFC\\Shi 그리고 Tomasi - 1994 - Good features to track.pdf:application/pdf}
}

@article{smith_susannew_1997,
	title = {{SUSAN}—A New Approach to Low Level Image Processing},
	volume = {23},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1023/A%3A1007963824710},
	doi = {10.1023/A:1007963824710},
	abstract = {This paper describes a new approach to low level image processing; in particular, edge and corner detection and structure preserving noise reduction. Non-linear filtering is used to define which parts of the image are closely related to each individual pixel; each pixel has associated with it a local image region which is of similar brightness to that pixel. The new feature detectors are based on the minimization of this local image region, and the noise reduction method uses this region as the smoothing neighbourhood. The resulting methods are accurate, noise resistant and fast. Details of the new feature detectors and of the new noise reduction method are described, along with test results.},
	language = {en},
	number = {1},
	urldate = {2014-10-31},
	journal = {International Journal of Computer Vision},
	author = {Smith, Stephen M. and Brady, J. Michael},
	month = may,
	year = {1997},
	keywords = {Artificial Intelligence (incl. Robotics), Automation and Robotics, Computer Imaging, Graphics and Computer Vision, Image Processing},
	pages = {45--78},
	file = {20060204_SUSAN_cap1.ppt:E\:\\Papers\\Zotero Repository\\storage\\UXBR8VDW\\20060204_SUSAN_cap1.ppt:application/msword;20060204_SUSAN_cap2.ppt:E\:\\Papers\\Zotero Repository\\storage\\2WG33DSK\\20060204_SUSAN_cap2.ppt:application/msword;20060204_SUSAN.ppt:E\:\\Papers\\Zotero Repository\\storage\\VJ2U3C4F\\20060204_SUSAN.ppt:application/msword;Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\UUHQ76EU\\Smith 그리고 Brady - 1997 - SUSAN—A New Approach to Low Level Image Processing.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\THWBURFH\\A1007963824710.html:text/html}
}

@article{mokhtarian_robust_1998,
	title = {Robust image corner detection through curvature scale space},
	volume = {20},
	issn = {0162-8828},
	doi = {10.1109/34.735812},
	abstract = {This paper describes a novel method for image corner detection based on the curvature scale-space ({CSS}) representation. The first step is to extract edges from the original image using a Canny detector (1986). The corner points of an image are defined as points where image edges have their maxima of absolute curvature. The corner points are detected at a high scale of the {CSS} and tracked through multiple lower scales to improve localization. This method is very robust to noise, and we believe that it performs better than the existing corner detectors An improvement to Canny edge detector's response to 45° and 135° edges is also proposed. Furthermore, the {CSS} detector can provide additional point features (curvature zero-crossings of image edge contours) in addition to the traditional corners},
	number = {12},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Mokhtarian, F. and Suomela, R.},
	month = dec,
	year = {1998},
	keywords = {absolute curvature maxima, Application software, Canny detector, Cascading style sheets, Computer vision, {CSS} representation, curvature scale space, curvature zero-crossings, Detectors, edge detection, edge extraction, feature extraction, Image edge detection, Noise robustness, object recognition, robust image corner detection, Stereo vision, tracking},
	pages = {1376--1381},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\AN4UWWN6\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\AFQIG483\\Mokhtarian 그리고 Suomela - 1998 - Robust image corner detection through curvature sc.pdf:application/pdf}
}

@article{trajkovic_fast_1998,
	title = {Fast corner detection},
	volume = {16},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885697000565},
	doi = {10.1016/S0262-8856(97)00056-5},
	abstract = {This paper describes a new corner detection algorithm, based on the property of corners that the change of image intensity should be high in all directions. Consequently, the corner response function ({CRF}) is computed as a minimum change of intensity over all possible directions. To compute the intensity change in an arbitrary direction an interpixel approximation is used. A multigrid approach is employed to reduce the computational complexity and to improve the quality of the detected corners. This algorithm, and other popular corner detectors, were evaluated and compared on the basis of their consistency, accuracy and speed using a range of images and video sequences. It was found that our algorithm performs well compared to the other algorithms, but it is significantly faster to compute.},
	number = {2},
	urldate = {2014-10-31},
	journal = {Image and Vision Computing},
	author = {Trajković, Miroslav and Hedley, Mark},
	month = feb,
	year = {1998},
	keywords = {Corner detector, feature detection, Multigrid algorithm},
	pages = {75--87},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\PASSWW6S\\Trajković 그리고 Hedley - 1998 - Fast corner detection.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\CMUIJCCD\\S0262885697000565.html:text/html}
}

@phdthesis{moravec_obstacle_1980,
	address = {Stanford, {CA}, {USA}},
	title = {Obstacle Avoidance and Navigation in the Real World by a Seeing Robot Rover},
	school = {Stanford University},
	author = {Moravec, Hans Peter},
	year = {1980},
	note = {{AAI}8024717},
	file = {Moravec - 1980 - Obstacle Avoidance and Navigation in the Real Worl.pdf:E\:\\Papers\\Zotero Repository\\storage\\UCCPSWWD\\Moravec - 1980 - Obstacle Avoidance and Navigation in the Real Worl.pdf:application/pdf}
}

@incollection{mikolajczyk_affine_2002,
	series = {Lecture Notes in Computer Science},
	title = {An Affine Invariant Interest Point Detector},
	copyright = {©2002 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-43745-1, 978-3-540-47969-7},
	url = {http://link.springer.com/chapter/10.1007/3-540-47969-4_9},
	abstract = {This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.},
	language = {en},
	number = {2350},
	urldate = {2014-11-03},
	booktitle = {Computer Vision — {ECCV} 2002},
	publisher = {Springer Berlin Heidelberg},
	author = {Mikolajczyk, Krystian and Schmid, Cordelia},
	editor = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
	month = jan,
	year = {2002},
	keywords = {Artificial Intelligence (incl. Robotics), Computer graphics, Image features, Image Processing and Computer Vision, matching, Pattern Recognition, recognition},
	pages = {128--142},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\JTMJTI8Q\\Mikolajczyk 그리고 Schmid - 2002 - An Affine Invariant Interest Point Detector.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\MCQGXW34\\3-540-47969-4_9.html:text/html}
}

@inproceedings{mikolajczyk_indexing_2001,
	title = {Indexing based on scale invariant interest points},
	volume = {1},
	doi = {10.1109/ICCV.2001.937561},
	abstract = {This paper presents a new method for detecting scale invariant interest points. The method is based on two recent results on scale space: (1) Interest points can be adapted to scale and give repeatable results (geometrically stable). (2) Local extrema over scale of normalized derivatives indicate the presence of characteristic local structures. Our method first computes a multi-scale representation for the Harris interest point detector. We then select points at which a local measure (the Laplacian) is maximal over scales. This allows a selection of distinctive points for which the characteristic scale is known. These points are invariant to scale, rotation and translation as well as robust to illumination changes and limited changes of viewpoint. For indexing, the image is characterized by a set of scale invariant points; the scale associated with each point allows the computation of a scale invariant descriptor. Our descriptors are, in addition, invariant to image rotation, of affine illumination changes and robust to small perspective deformations. Experimental results for indexing show an excellent performance up to a scale factor of 4 for a database with more than 5000 images},
	booktitle = {Eighth {IEEE} International Conference on Computer Vision, 2001. {ICCV} 2001. Proceedings},
	author = {Mikolajczyk, K. and Schmid, C.},
	year = {2001},
	keywords = {affine illumination changes, Computer vision, database indexing, Detectors, Filters, Harris interest point detector, Image databases, indexing, Laplace equations, Layout, Lighting, multi-scale representation, normalized derivatives, Robustness, scale invariant interest points, scale invariant points},
	pages = {525--531 vol.1},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\8UFT5QGC\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\BPCIRUMJ\\Mikolajczyk 그리고 Schmid - 2001 - Indexing based on scale invariant interest points.pdf:application/pdf}
}

@article{kitchen_gray-level_1982,
	title = {Gray-level corner detection},
	volume = {1},
	issn = {0167-8655},
	url = {http://www.sciencedirect.com/science/article/pii/0167865582900204},
	doi = {10.1016/0167-8655(82)90020-4},
	abstract = {The usual approach to detecting corners in shapes involves first segmenting the shape, then locating the corners in its boundary. We present several techniques for measuring ‘cornerity’ values in gray-level images, without prior segmentation, so that corners can be detected by thresholding these values.},
	number = {2},
	urldate = {2014-11-03},
	journal = {Pattern Recognition Letters},
	author = {Kitchen, Les and Rosenfeld, Azriel},
	month = dec,
	year = {1982},
	keywords = {corner detection, Digital image processing, gray-level images, segmentation, shape recognition},
	pages = {95--102},
	file = {ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\2JG6JHNH\\0167865582900204.html:text/html}
}

@inproceedings{beaudet_rotationally_1978,
	title = {Rotationally invariant image operators},
	booktitle = {Proceedings of the International Joint Conference on Pattern Recognition},
	author = {Beaudet, Paul R},
	year = {1978},
	pages = {579--583}
}

@phdthesis{noble_descriptions_1989,
	title = {Descriptions of image surfaces.},
	school = {University of Oxford},
	author = {Noble, Julia Alison},
	year = {1989}
}

@article{kenney_condition_2003,
	title = {A condition number for point matching with application to registration and postregistration error estimation},
	volume = {25},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2003.1240118},
	abstract = {Selecting salient points from two or more images for computing correspondence is a well-studied problem in image analysis. This paper describes a new and effective technique for selecting these tiepoints using condition numbers, with application to image registration and mosaicking. Condition numbers are derived for point-matching methods based on minimizing windowed objective functions for 1) translation, 2) rotation-scaling-translation ({RST}), and 3) affine transformations. Our principal result is that the condition numbers satisfy {KTrans} ≤ {KRST} ≤ {KAffine}. That is, if a point is ill-conditioned with respect to point-matching via translation, then it is also unsuited for matching with respect to {RST} and affine transforms. This is fortunate since {KTrans} is easily computed whereas {KRST} and {KAffine} are not. The second half of the paper applies the condition estimation results to the problem of identifying tiepoints in pairs of images for the purpose of registration. Once these points have been matched (after culling outliers using a {RANSAC}-like procedure), the registration parameters are computed. The postregistration error between the reference image and the stabilized image is then estimated by evaluating the translation between these images at points exhibiting good conditioning with respect to translation. The proposed method of tiepoint selection and matching using condition number provides a reliable basis for registration. The method has been tested on a large number of diverse collection of images - multidate Landsat images, aerial images, aerial videos, and infrared images. A Web site where the users can try our registration software is available and is being actively used by researchers around the world.},
	number = {11},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kenney, C.S. and Manjunath, B.S. and Zuliani, M. and Hewer, G.A. and Van Nevel, A.},
	month = nov,
	year = {2003},
	keywords = {correspondence, Detectors, Eigenvalues and eigenfunctions, Error analysis, error estimation, feature extraction, feature representation, image analysis, image matching, image motion analysis, image points, image registration, image sequences, Infrared imaging, Remote sensing, Satellites, Testing, Videos},
	pages = {1437--1454},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\TTVISIMA\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\XQUB3GKD\\Kenney et al. - 2003 - A condition number for point matching with applica.pdf:application/pdf}
}

@inproceedings{zuliani_mathematical_2004,
	title = {A Mathematical Comparison of Point Detectors},
	doi = {10.1109/CVPR.2004.8},
	abstract = {Selecting salient points from two or more images for computing correspondences is a fundamental problem in image analysis. Three methods originally proposed by Harris et al. in [A combined corner and edge detector], by Noble et al. in [Descriptions of image surfaces] and by Shi et al. in [Good features to track] proved to be quite effective and robust and have been widely used by the computer vision community. The goal of this paper is to analyze these point detectors starting from the algebraic and numerical properties of the image auto-correlation matrix. To accomplish this task we will first introduce a "natural" constraint that needs to be satisfied by any point detector based on the auto-correlation matrix. Then, by casting the point detection problem in a mathematical framework based on condition theory [A condition number for point matching with application to registration and post-registration error estimation], we will show that under certain hypothesis the point detectors are equivalent modulo the choice of a specific matrix norm. The results presented in this paper will provide a novel unifying description for the most commonly used point detection algorithms.},
	booktitle = {Conference on Computer Vision and Pattern Recognition Workshop, 2004. {CVPRW} '04},
	author = {Zuliani, M. and Kenney, C. and Manjunath, B.S.},
	month = jun,
	year = {2004},
	keywords = {Autocorrelation, Casting, Computer vision, Detection algorithms, Detectors, image analysis, image reconstruction, Layout, Robustness, Transmission line matrix methods},
	pages = {172--172},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\W3RWWZ3A\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\S2E77KTU\\Zuliani et al. - 2004 - A Mathematical Comparison of Point Detectors.pdf:application/pdf}
}

@inproceedings{lucas_iterative_1981,
	title = {An iterative image registration technique with an application to stereo vision.},
	booktitle = {Proceedings of {DARPA} Image Understanding Workshop},
	author = {Lucas, Bruce D and Kanade, Takeo},
	month = apr,
	year = {1981},
	pages = {121--130},
	file = {Lucas 그리고 Kanade - 1981 - An iterative image registration technique with an .pdf:E\:\\Papers\\Zotero Repository\\storage\\WKM4TG5B\\Lucas 그리고 Kanade - 1981 - An iterative image registration technique with an .pdf:application/pdf}
}

@article{rosten_faster_2010,
	title = {Faster and Better: A Machine Learning Approach to Corner Detection},
	volume = {32},
	issn = {0162-8828},
	shorttitle = {Faster and Better},
	doi = {10.1109/TPAMI.2008.275},
	abstract = {The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live {PAL} video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, {SIFT} 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.},
	number = {1},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rosten, Edward and Porter, R. and Drummond, Tom},
	month = jan,
	year = {2010},
	keywords = {3D location, Computer vision, corner detection, Detectors, edge detection, edge technique, feature detection, feature detection., feature extraction, Image edge detection, image matching, Image recognition, Layout, learning (artificial intelligence), Machine learning, machine learning approach, {PAL} video, real-world application, simultaneous localization and mapping, Streaming media, Testing},
	pages = {105--119},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\W7QQPANU\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\7CHMVZ7G\\Rosten et al. - 2010 - Faster and Better A Machine Learning Approach to .pdf:application/pdf}
}

@article{lindeberg_scale-space_1994,
	title = {Scale-space theory: A basic tool for analysing structures at different scales},
	volume = {21},
	shorttitle = {Scale-space theory},
	abstract = {An inherent property of objects in the world is that they only exist as meaningful entities over certain ranges of scale. If one aims at describing the structure of unknown real-world signals, then a multi-scale representation of data is of crucial importance. This chapter gives a tutorial review of a special type of multi-scale representation, linear scale-space representation, which has been developed by the computer vision community in order to handle image structures at different scales in a consistent manner. The basic idea is to embed the original signal into a oneparameter family of gradually smoothed signals, in which the fine scale details are successively suppressed. Under rather general conditions on the type of computations that are to performed at the first stages of visual processing, in what can be termed the visual front end, it can be shown that the Gaussian kernel and its derivatives are singled out as the only possible smoothing kernels. The conditions that specify ...},
	journal = {Journal of Applied Statistics},
	author = {Lindeberg, Tony},
	year = {1994},
	pages = {224--270},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IRWB9W2H\\Lindeberg - 1994 - Scale-space theory A basic tool for analysing str.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\4P554N85\\summary.html:text/html}
}

@inproceedings{crowley_fast_2003,
	title = {Fast Computation of Characteristic Scale Using a Half-Octave Pyramid},
	abstract = {The characteristic (or intrinsic) scale of a local image pattern is the scale parameter at which the Laplacian provides a local maximum. Nearly every position in an image will exhibit a small number of such characteristic scales. Computing a Gaussian jet at a characteristic scale provides a scale invariant feature vector for tracking, matching, indexing and recognition. However, the computational cost of directly searching the scale axis for the characteristic scale at each image position can be prohibitively expensive.},
	booktitle = {In: Scale Space 03: 4th International Conference on Scale-Space theories in Computer Vision, Isle of Skye},
	author = {Crowley, James L. and Riff, Olivier and Piater, Justus H.},
	year = {2003},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\CBF969RA\\Crowley et al. - 2002 - Fast Computation of Characteristic Scale Using a H.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\WU2UBMMJ\\summary.html:text/html}
}

@inproceedings{brown_invariant_2002,
	title = {Invariant Features from Interest Point Groups},
	abstract = {This paper approaches the problem of finding correspondences between images in which there are large changes in viewpoint, scale and illumination. Recent work has shown that scale-space `interest points' may be found with good repeatability in spite of such changes. Furthermore, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descriptors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process.},
	booktitle = {In British Machine Vision Conference},
	author = {Brown, Matthew and Lowe, David},
	year = {2002},
	pages = {656--665},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\6X235DWI\\Brown 그리고 Lowe - 2002 - Invariant Features from Interest Point Groups.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\3C3GEJZ5\\summary.html:text/html}
}

@incollection{schaffalitzky_multi-view_2002,
	series = {Lecture Notes in Computer Science},
	title = {Multi-view Matching for Unordered Image Sets, or “How Do I Organize My Holiday Snaps?”},
	copyright = {©2002 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-43745-1, 978-3-540-47969-7},
	shorttitle = {Multi-view Matching for Unordered Image Sets, or “How Do I Organize My Holiday Snaps?},
	url = {http://link.springer.com/chapter/10.1007/3-540-47969-4_28},
	abstract = {There has been considerable success in automated reconstruction for image sequences where small baseline algorithms can be used to establish matches across a number of images. In contrast in the case of widely separated views, methods have generally been restricted to two or three views. In this paper we investigate the problem of establishing relative viewpoints given a large number of images where no ordering information is provided. A typical application would be where images are obtained from different sources or at different times: both the viewpoint (position, orientation, scale) and lighting conditions may vary significantly over the data set. Such a problem is not fundamentally amenable to exhaustive pair wise and triplet wide baseline matching because this would be prohibitively expensive as the number of views increases. Instead, we investiate how a combination of image invariants, covariants, and multiple view relations can be used in concord to enable efficient multiple view matching. The result is a matching algorithm which is linear in the number of views. The methods are illustrated on several real image data sets. The output enables an image based technique for navigating in a 3D scene, moving from one image to whichever image is the next most appropriate.},
	language = {en},
	number = {2350},
	urldate = {2014-11-06},
	booktitle = {Computer Vision — {ECCV} 2002},
	publisher = {Springer Berlin Heidelberg},
	author = {Schaffalitzky, F. and Zisserman, A.},
	editor = {Heyden, Anders and Sparr, Gunnar and Nielsen, Mads and Johansen, Peter},
	month = jan,
	year = {2002},
	keywords = {Artificial Intelligence (incl. Robotics), Computer graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {414--431},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IUEH3H6A\\Schaffalitzky 그리고 Zisserman - 2002 - Multi-view Matching for Unordered Image Sets, or “.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\F263JXDJ\\3-540-47969-4_28.html:text/html}
}

@inproceedings{tuytelaars_wide_2000,
	title = {Wide Baseline Stereo Matching based on Local, Affinely Invariant Regions},
	abstract = {`Invariant regions' are image patches that automatically deform with changing viewpoint as to keep on covering identical physical parts of a scene. Such regions are then described by a set of invariant features, which makes it relatively easy to match them between views and under changing illumination. In previous work, we have presented invariant regions that are based on a combination of corners and edges. The application discussed then was image database retrieval. Here, an alternative method for extracting (affinely) invariant regions is given, that does not depend on the presence of edges or corners in the image but is purely intensity-based. Also, we demonstrate the use of such regions for another application, which is wide baseline stereo matching. As a matter of fact, the goal is to build an opportunistic system that exploits several types of invariant regions as it sees fit. This yields more correspondences and a system that can deal with a wider range of images. To increase t...},
	booktitle = {In Proc. {BMVC}},
	author = {Tuytelaars, Tinne and Gool, Luc Van},
	year = {2000},
	pages = {412--425},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\PXR4MCCP\\Tuytelaars 그리고 Gool - 2000 - Wide Baseline Stereo Matching based on Local, Affi.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\CIRQZX2Q\\summary.html:text/html}
}

@article{matas_robust_2004,
	series = {British Machine Vision Computing 2002},
	title = {Robust wide-baseline stereo from maximally stable extremal regions},
	volume = {22},
	issn = {0262-8856},
	url = {http://www.sciencedirect.com/science/article/pii/S0262885604000435},
	doi = {10.1016/j.imavis.2004.02.006},
	abstract = {The wide-baseline stereo problem, i.e. the problem of establishing correspondences between a pair of images taken from different viewpoints is studied.

A new set of image elements that are put into correspondence, the so called extremal regions, is introduced. Extremal regions possess highly desirable properties: the set is closed under (1) continuous (and thus projective) transformation of image coordinates and (2) monotonic transformation of image intensities. An efficient (near linear complexity) and practically fast detection algorithm (near frame rate) is presented for an affinely invariant stable subset of extremal regions, the maximally stable extremal regions ({MSER}).

A new robust similarity measure for establishing tentative correspondences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from extremal regions), some that are significantly larger (and hence discriminative) than the {MSERs}, may be used to establish tentative correspondences.

The high utility of {MSERs}, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5×), illumination conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test problems. Good estimates of epipolar geometry (average distance from corresponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained.},
	number = {10},
	urldate = {2014-11-06},
	journal = {Image and Vision Computing},
	author = {Matas, J and Chum, O and Urban, M and Pajdla, T},
	month = sep,
	year = {2004},
	keywords = {Distinguished regions, Maximally stable extremal regions, {MSER}, Robust metric, Wide-baseline stereo},
	pages = {761--767},
	file = {ScienceDirect Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\34JDEFKX\\Matas et al. - 2004 - Robust wide-baseline stereo from maximally stable .pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\S2PQWVW8\\S0262885604000435.html:text/html}
}

@incollection{kadir_affine_2004,
	series = {Lecture Notes in Computer Science},
	title = {An Affine Invariant Salient Region Detector},
	copyright = {©2004 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-21984-2, 978-3-540-24670-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-24670-1_18},
	abstract = {In this paper we describe a novel technique for detecting salient regions in an image. The detector is a generalization to affine invariance of the method introduced by Kadir and Brady [10]. The detector deems a region salient if it exhibits unpredictability in both its attributes and its spatial scale. The detector has significantly different properties to operators based on kernel convolution, and we examine three aspects of its behaviour: invariance to viewpoint change; insensitivity to image perturbations; and repeatability under intra-class variation. Previous work has, on the whole, concentrated on viewpoint invariance. A second contribution of this paper is to propose a performance test for evaluating the two other aspects. We compare the performance of the saliency detector to other standard detectors including an affine invariance interest point detector. It is demonstrated that the saliency detector has comparable viewpoint invariance performance, but superior insensitivity to perturbations and intra-class variation performance for images of certain object classes.},
	language = {en},
	number = {3021},
	urldate = {2014-11-06},
	booktitle = {Computer Vision - {ECCV} 2004},
	publisher = {Springer Berlin Heidelberg},
	author = {Kadir, Timor and Zisserman, Andrew and Brady, Michael},
	editor = {Pajdla, Tomás and Matas, Jiří},
	month = jan,
	year = {2004},
	keywords = {Artificial Intelligence (incl. Robotics), Computer graphics, Image Processing and Computer Vision, Pattern Recognition},
	pages = {228--241},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\GPQQRPE4\\Kadir et al. - 2004 - An Affine Invariant Salient Region Detector.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\457D9S9S\\978-3-540-24670-1_18.html:text/html}
}

@article{mikolajczyk_comparison_2005,
	title = {A Comparison of Affine Region Detectors},
	volume = {65},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1007/s11263-005-3848-x},
	doi = {10.1007/s11263-005-3848-x},
	abstract = {The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris (Mikolajczyk and Schmid, 2002; Schaffalitzky and Zisserman, 2002) and Hessian points (Mikolajczyk and Schmid, 2002), a detector of ‘maximally stable extremal regions', proposed by Matas et al. (2002); an edge-based region detector (Tuytelaars and Van Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van Gool, 2000), and a detector of ‘salient regions', proposed by Kadir, Zisserman and Brady (2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.},
	language = {en},
	number = {1-2},
	urldate = {2014-11-06},
	journal = {International Journal of Computer Vision},
	author = {Mikolajczyk, K. and Tuytelaars, T. and Schmid, C. and Zisserman, A. and Matas, J. and Schaffalitzky, F. and Kadir, T. and Gool, L. Van},
	month = nov,
	year = {2005},
	keywords = {affine region detectors, Artificial Intelligence (incl. Robotics), Computer Imaging, Graphics and Computer Vision, Image Processing, invariant image description, Local features, Pattern Recognition, performance evaluation},
	pages = {43--72},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\P7IHKZ27\\Mikolajczyk et al. - 2005 - A Comparison of Affine Region Detectors.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\4P7K29GM\\s11263-005-3848-x.html:text/html}
}

@article{moreels_evaluation_2007,
	title = {Evaluation of Features Detectors and Descriptors based on 3D Objects},
	volume = {73},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/article/10.1007/s11263-006-9967-1},
	doi = {10.1007/s11263-006-9967-1},
	abstract = {We explore the performance of a number of popular feature detectors and descriptors in matching 3D object features across viewpoints and lighting conditions. To this end we design a method, based on intersecting epipolar constraints, for providing ground truth correspondence automatically. These correspondences are based purely on geometric information, and do not rely on the choice of a specific feature appearance descriptor. We test detector-descriptor combinations on a database of 100 objects viewed from 144 calibrated viewpoints under three different lighting conditions. We find that the combination of Hessian-affine feature finder and {SIFT} features is most robust to viewpoint change. Harris-affine combined with {SIFT} and Hessian-affine combined with shape context descriptors were best respectively for lighting change and change in camera focal length. We also find that no detector-descriptor combination performs well with viewpoint changes of more than 25–30∘.},
	language = {en},
	number = {3},
	urldate = {2014-11-06},
	journal = {International Journal of Computer Vision},
	author = {Moreels, Pierre and Perona, Pietro},
	month = jul,
	year = {2007},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Imaging, Vision, Pattern Recognition and Graphics, features descriptors, features detectors, Image Processing and Computer Vision, object recognition, Pattern Recognition},
	pages = {263--284},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\GHA3AX9W\\Moreels 그리고 Perona - 2007 - Evaluation of Features Detectors and Descriptors b.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\SMCKDXGP\\s11263-006-9967-1.html:text/html}
}

@inproceedings{foo_pruning_2007,
	address = {Darlinghurst, Australia, Australia},
	series = {{ADC} '07},
	title = {Pruning {SIFT} for Scalable Near-duplicate Image Matching},
	isbn = {1-920-68244-9},
	url = {http://dl.acm.org/citation.cfm?id=1273730.1273738},
	abstract = {The detection of image versions from large image collections is a formidable task as two images are rarely identical. Geometric variations such as cropping, rotation, and slight photometric alteration are unsuitable for content-based retrieval techniques, whereas digital watermarking techniques have limited application for practical retrieval. Recently, the application of Scale Invariant Feature Transform ({SIFT}) interest points to this domain have shown high effectiveness, but scalability remains a problem due to the large number of features generated for each image. In this work, we show that for this application domain, the {SIFT} interest points can be dramatically pruned to effect large reductions in both memory requirements and query run-time, with almost negligible loss in effectiveness. We demonstrate that, unlike using the original {SIFT} features, the pruned features scales better for collections containing hundreds of thousands of images.},
	urldate = {2014-11-17},
	booktitle = {Proceedings of the Eighteenth Conference on Australasian Database - Volume 63},
	publisher = {Australian Computer Society, Inc.},
	author = {Foo, Jun Jie and Sinha, Ranjan},
	year = {2007},
	keywords = {near-duplicate image matching, near-replicate image retrieval},
	pages = {63--71},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\X7QP5ZDI\\Foo 그리고 Sinha - 2007 - Pruning SIFT for Scalable Near-duplicate Image Mat.pdf:application/pdf}
}

@inproceedings{brown_invariant_2002-1,
	title = {Invariant Features from Interest Point Groups},
	abstract = {This paper approaches the problem of finding correspondences between images in which there are large changes in viewpoint, scale and illumination. Recent work has shown that scale-space `interest points' may be found with good repeatability in spite of such changes. Furthermore, the high entropy of the surrounding image regions means that local descriptors are highly discriminative for matching. For descriptors at interest points to be robustly matched between images, they must be as far as possible invariant to the imaging process.},
	booktitle = {In British Machine Vision Conference},
	author = {Brown, Matthew and Lowe, David},
	year = {2002},
	pages = {656--665},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\TQJFEI8D\\Brown 그리고 Lowe - 2002 - Invariant Features from Interest Point Groups.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\X5472RJH\\summary.html:text/html}
}

@inproceedings{neubeck_efficient_2006,
	title = {Efficient Non-Maximum Suppression},
	volume = {3},
	doi = {10.1109/ICPR.2006.479},
	abstract = {In this work we scrutinize a low level computer vision task - non-maximum suppression ({NMS}) - which is a crucial preprocessing step in many computer vision applications. Especially in real time scenarios, efficient algorithms for such preprocessing algorithms, which operate on the full image resolution, are important. In the case of {NMS}, it seems that merely the straightforward implementation or slight improvements are known. We show that these are far from being optimal, and derive several algorithms ranging from easy-to-implement to highly-efficient},
	booktitle = {18th International Conference on Pattern Recognition, 2006. {ICPR} 2006},
	author = {Neubeck, A. and Van Gool, L.},
	year = {2006},
	keywords = {Application software, Computer vision, Data mining, Filters, Gas insulated transmission lines, image reconstruction, image resolution, Image texture analysis, nonmaximum suppression, object recognition, Pixel},
	pages = {850--855},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\FT9CCFBT\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\MGG3FT6M\\Neubeck 그리고 Van Gool - 2006 - Efficient Non-Maximum Suppression.pdf:application/pdf}
}

@inproceedings{bleser_advanced_2008,
	title = {Advanced tracking through efficient image processing and visual-inertial sensor fusion},
	doi = {10.1109/VR.2008.4480765},
	abstract = {We present a new visual-inertial tracking device for augmented and virtual reality applications. The paper addresses two fundamental issues of such systems. The first one concerns the definition and modelling of the sensor fusion. Much work has been done in this area and several models for exploiting the data of the gyroscopes and linear accelerometers have been proposed. However, the respective advantages of each model and in particular the benefits of the integration of the accelerometer data in the filter are still unclear. The paper therefore provides an evaluation of different models with special investigation of the effects of using accelerometers on the tracking performance. The second contribution is about the development of an image processing approach that does not require special landmarks but uses natural features. Our solution relies on a 3D model of the scene that enables to predict the appearances of the features by rendering the model using the prediction data of the sensor fusion filter. The feature localisation is robust and accurate mainly because local lighting is also estimated. The final system is evaluated with help of ground-truth and real data. High stability and accuracy is demonstrated also for large environments.},
	booktitle = {{IEEE} Virtual Reality Conference, 2008. {VR} '08},
	author = {Bleser, G. and Stricker, D.},
	month = mar,
	year = {2008},
	keywords = {1.4.8 [Image processing and computer vision]: Scene analysis-Motion, Photometry, Sensor fusion, Tracking, 3D scene model, accelerometer, Accelerometers, Algorithms, augmented reality, Experimentation, extended, feature localisation, filtering theory, Filters, G. 3 [Probability and statistics]: Markov processes, Probabilistic algorithms, Robust regression, Stochastic processes, Gyroscopes, I.2.9 [Artificial intelligence]: Robotics?? Kinematics and dynamics, Sensors, I.2.10 [Artificial intelligence]: Vision and Scene Understanding 3D/stereo scene analysis, Intensity, color, photometry, and thresholding, Motion, Texture, Video analysis, I.3.m [Computer graphics]: Miscellaneous??Augmented Reality, I.5.5 [Pattern recognition]: Applications??Computer vision, Image Processing, inertial sensors, Layout, local lighting, markerless camera tracking, model-based tracking, optical tracking, Performance, Predictive models, rendering, rendering (computer graphics), Robustness, sensor fusion, sensor fusion filter, solid modelling, Theory, Verification, virtual reality, visual-inertial sensor fusion, visual-inertial tracking device},
	pages = {137--144},
	file = {Bleser 그리고 Stricker - 2008 - Advanced tracking through efficient image processi.pdf:E\:\\Papers\\Zotero Repository\\storage\\RQUHRX6R\\Bleser 그리고 Stricker - 2008 - Advanced tracking through efficient image processi.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\6FRFF8Q9\\articleDetails.html:text/html}
}

@incollection{carrera_robust_2007,
	series = {Lecture Notes in Computer Science},
	title = {Robust Feature Descriptors for Efficient Vision-Based Tracking},
	copyright = {©2007 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-76724-4, 978-3-540-76725-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-76725-1_27},
	abstract = {This paper presents a robust implementation of an object tracker able to tolerate partial occlusions, rotation and scale for a variety of different objects. The objects are represented by collections of interest points which are described in a multi-resolution framework, giving a representation of those points at different scales. Inspired by [1], a stack of descriptors is built only the first time that the interest points are detected and extracted from the region of interest. This provides efficiency of representation and results in faster tracking due to the fact that it can be done off-line. An Unscented Kalman Filter ({UKF}) using a constant velocity model estimates the position and the scale of the object, with the uncertainty in the position and the scale obtained by the {UKF}, the search of the object can be constrained only in a specific region in both the image and in scale. The use of this approach shows an improvement in real-time tracking and in the ability to recover from full occlusions.},
	language = {en},
	number = {4756},
	urldate = {2014-09-11},
	booktitle = {Progress in Pattern Recognition, Image Analysis and Applications},
	publisher = {Springer Berlin Heidelberg},
	author = {Carrera, Gerardo and Savage, Jesus and Mayol-Cuevas, Walterio},
	editor = {Rueda, Luis and Mery, Domingo and Kittler, Josef},
	month = jan,
	year = {2007},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Imaging, Vision, Pattern Recognition and Graphics, Harris detector, Image Processing and Computer Vision, object tracking, Pattern Recognition, Speeded-Up Robust Features ({SURF}), Unscented Kalman Filter},
	pages = {251--260},
	file = {Carrera et al. - 2007 - Robust Feature Descriptors for Efficient Vision-Ba.pdf:E\:\\Papers\\Zotero Repository\\storage\\ACUTMVIM\\Carrera et al. - 2007 - Robust Feature Descriptors for Efficient Vision-Ba.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\5K7NR74C\\10.html:text/html}
}

@inproceedings{chekhlov_robust_2007,
	title = {Robust Real-Time Visual {SLAM} Using Scale Prediction and Exemplar Based Feature Description},
	doi = {10.1109/CVPR.2007.383026},
	abstract = {Two major limitations of real-time visual {SLAM} algorithms are the restricted range of views over which they can operate and their lack of robustness when faced with erratic camera motion or severe visual occlusion. In this paper we describe a visual {SLAM} algorithm which addresses both of these problems. The key component is a novel feature description method which is both fast and capable of repeat-able correspondence matching over a wide range of viewing angles and scales. This is achieved in real-time by using a {SIFT}-like spatial gradient descriptor in conjunction with efficient scale prediction and exemplar based feature representation. Results are presented illustrating robust realtime {SLAM} operation within an office environment.},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, 2007. {CVPR} '07},
	author = {Chekhlov, D. and Pupilli, M. and Mayol, W. and Calway, A},
	month = jun,
	year = {2007},
	keywords = {Cameras, Computer science, erratic camera motion, Feature description, feature extraction, Filtering, gradient methods, image matching, image representation, Layout, Matched filters, mobile robots, repeatable correspondence matching, robot vision, Robot vision systems, Robustness, robust real-time visual {SLAM}, scale prediction, {SIFT}-like spatial gradient descriptor, simultaneous localization and mapping, {SLAM} (robots), Stochastic processes, visual occlusion, Wearable computers},
	pages = {1--7},
	file = {Chekhlov et al. - 2007 - Robust Real-Time Visual SLAM Using Scale Predictio.pdf:E\:\\Papers\\Zotero Repository\\storage\\NAVRVVC2\\Chekhlov et al. - 2007 - Robust Real-Time Visual SLAM Using Scale Predictio.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\EXRCFFMU\\articleDetails.html:text/html}
}

@article{cheng_visual_2006,
	title = {Visual odometry on the Mars exploration rovers - a tool to ensure accurate driving and science imaging},
	volume = {13},
	issn = {1070-9932},
	doi = {10.1109/MRA.2006.1638016},
	abstract = {In this paper, visual odometry is presented as an approach to position estimation to find features in a stereo image pair and track them from one frame to the next. Visual odometry has been a highly effective tool for maintaining vehicle safety while driving near obstacles on slopes, achieving difficult drive approaches in fewer sols, and ensuring accurate science imaging. Although it requires active pointing by human drivers in feature-poor terrain, the improved position knowledge enables more autonomous capability and better science return during planetary operations},
	number = {2},
	journal = {{IEEE} Robotics Automation Magazine},
	author = {Cheng, Yang and Maimone, M.W. and Matthies, L.},
	month = jun,
	year = {2006},
	keywords = {aerospace engineering, aerospace robotics, Computer vision, Instruments, Mars, Mars exploration rovers, Maximum likelihood estimation, mobile robots, motion estimation, planetary operations, planetary rovers, position estimation, Position measurement, Robotics and automation, Soil, stereo image, stereo image processing, telerobotics, tracking, Vehicle driving, vehicle safety, visual odometry, Wheels},
	pages = {54--62},
	file = {Cheng et al. - 2006 - Visual odometry on the Mars exploration rovers - a.pdf:E\:\\Papers\\Zotero Repository\\storage\\N7MBTXII\\Cheng et al. - 2006 - Visual odometry on the Mars exploration rovers - a.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\AFVXZQ2C\\articleDetails.html:text/html}
}

@article{davison_monoslam:_2007,
	title = {{MonoSLAM}: Real-Time Single Camera {SLAM}},
	volume = {29},
	issn = {0162-8828},
	shorttitle = {{MonoSLAM}},
	doi = {10.1109/TPAMI.2007.1049},
	abstract = {We present a real-time algorithm which can recover the 3D trajectory of a monocular camera, moving rapidly through a previously unknown scene. Our system, which we dub {MonoSLAM}, is the first successful application of the {SLAM} methodology from mobile robotics to the "pure vision" domain of a single uncontrolled camera, achieving real time but drift-free performance inaccessible to structure from motion approaches. The core of the approach is the online creation of a sparse but persistent map of natural landmarks within a probabilistic framework. Our key novel contributions include an active approach to mapping and measurement, the use of a general motion model for smooth camera movement, and solutions for monocular feature initialization and feature orientation estimation. Together, these add up to an extremely efficient and robust algorithm which runs at 30 Hz with standard {PC} and camera hardware. This work extends the range of robotic systems in which {SLAM} can be usefully applied, but also opens up new areas. We present applications of {MonoSLAM} to real-time 3D localization and mapping for a high-performance full-size humanoid robot and live augmented reality with a hand-held camera},
	number = {6},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Davison, AJ. and Reid, ID. and Molton, N.D. and Stasse, O.},
	month = jun,
	year = {2007},
	keywords = {3D/stereo scene analysis, 3D trajectory, Algorithms, Artificial Intelligence, Autonomous vehicles, Cameras, Computer Systems, feature extraction, feature orientation estimation, Hardware, humanoid robot, humanoid robots, Image Enhancement, Image Interpretation, Computer-Assisted, image sensors, Imaging, Three-Dimensional, Information Storage and Retrieval, Layout, mobile robots, monocular camera, {MonoSLAM}, motion estimation, Motion measurement, Numerical Analysis, Computer-Assisted, Pattern Recognition, Automated, Photogrammetry, real-time single camera {SLAM}, Real time systems, Robot vision systems, Robustness, Signal Processing, Computer-Assisted, simultaneous localization and mapping, {SLAM} (robots), tracking., Video Recording},
	pages = {1052--1067},
	file = {Davison et al. - 2007 - MonoSLAM Real-Time Single Camera SLAM.pdf:E\:\\Papers\\Zotero Repository\\storage\\XKJ988WN\\Davison et al. - 2007 - MonoSLAM Real-Time Single Camera SLAM.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\42EASFK9\\articleDetails.html:text/html}
}

@inproceedings{diverdi_envisor:_2008,
	title = {Envisor: Online Environment Map Construction for Mixed Reality},
	shorttitle = {Envisor},
	doi = {10.1109/VR.2008.4480745},
	abstract = {One of the main goals of anywhere augmentation is the development of automatic algorithms for scene acquisition in augmented reality systems. In this paper, we present Envisor, a system for online construction of environment maps in new locations. To accomplish this, Envisor uses vision-based frame to frame and landmark orientation tracking for long-term, drift-free registration. For additional robustness, a gyroscope/compass orientation unit can optionally be used for hybrid tracking. The tracked video is then projected into a cubemap frame by frame. Feedback is presented to the user to help avoid gaps in the cubemap, while any remaining gaps are filled by texture diffusion. The resulting environment map can be used for a variety of applications, including shading of virtual geometry and remote presence.},
	booktitle = {{IEEE} Virtual Reality Conference, 2008. {VR} '08},
	author = {DiVerdi, S. and Wither, J. and Hollerei, T.},
	month = mar,
	year = {2008},
	keywords = {augmented reality, Cameras, Computer graphics, Computer vision, drift-free registration, environment map, Envisor, Feedback, Geometry, Gyroscopes, I.3.7 [Computer Graphics]: Three-Dimensional Graphics and {RealismÂ}¿Virtual Reality, I.4.8 [Image Processing and Computer Vision]: Scene {AnalysisÂ}¿Tracking, image registration, landmark orientation tracking, Layout, online environment map construction, Robustness, scene acquisition, tracking, virtual geometry, virtual reality},
	pages = {19--26},
	file = {DiVerdi et al. - 2008 - Envisor Online Environment Map Construction for M.pdf:E\:\\Papers\\Zotero Repository\\storage\\AWA2GNRT\\DiVerdi et al. - 2008 - Envisor Online Environment Map Construction for M.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\N3FNPSNI\\articleDetails.html:text/html}
}

@inproceedings{eade_scalable_2006,
	title = {Scalable Monocular {SLAM}},
	volume = {1},
	doi = {10.1109/CVPR.2006.263},
	abstract = {Localization and mapping in unknown environments becomes more difficult as the complexity of the environment increases. With conventional techniques, the cost of maintaining estimates rises rapidly with the number of landmarks mapped. We present a monocular {SLAM} system that employs a particle filter and top-down search to allow realtime performance while mapping large numbers of landmarks. To our knowledge, we are the first to apply this {FastSLAM}-type particle filter to single-camera {SLAM}. We also introduce a novel partial initialization procedure that efficiently determines the depth of new landmarks. Moreover, we use information available in observations of new landmarks to improve camera pose estimates. Results show the system operating in real-time on a standard workstation while mapping hundreds of landmarks.},
	booktitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Eade, E. and Drummond, Tom},
	month = jun,
	year = {2006},
	keywords = {Bayesian methods, Cameras, Computer vision, Costs, Filtering, Navigation, Particle filters, Recursive estimation, Robots, simultaneous localization and mapping},
	pages = {469--476},
	file = {Eade 그리고 Drummond - 2006 - Scalable Monocular SLAM.pdf:E\:\\Papers\\Zotero Repository\\storage\\K43PWZG9\\Eade 그리고 Drummond - 2006 - Scalable Monocular SLAM.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\6WTF9XJN\\articleDetails.html:text/html}
}

@inproceedings{klein_parallel_2007,
	title = {Parallel Tracking and Mapping for Small {AR} Workspaces},
	doi = {10.1109/ISMAR.2007.4538852},
	abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting {SLAM} algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small {AR} workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems.},
	booktitle = {6th {IEEE} and {ACM} International Symposium on Mixed and Augmented Reality, 2007. {ISMAR} 2007},
	author = {Klein, G. and Murray, D.},
	month = nov,
	year = {2007},
	keywords = {Algorithm design and analysis, augmented reality, batch optimisation techniques, Cameras, Concurrent computing, hand-held camera, Handheld computers, Layout, parallel mapping, parallel tracking, robotic exploration, robot vision, Robot vision systems, Robustness, simultaneous localization and mapping, {SLAM} algorithms, {SLAM} (robots), tracking, Yarn},
	pages = {225--234},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\86KBZSAE\\abs_all.html:text/html;Klein 그리고 Murray - 2007 - Parallel Tracking and Mapping for Small AR Workspa.pdf:E\:\\Papers\\Zotero Repository\\storage\\JS8E6DB4\\Klein 그리고 Murray - 2007 - Parallel Tracking and Mapping for Small AR Workspa.pdf:application/pdf}
}

@inproceedings{lee_hybrid_2008,
	title = {Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality},
	doi = {10.1109/VR.2008.4480766},
	abstract = {We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality ({AR}) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking methods. Distinctive image features of the scene are detected and tracked frame- to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a multi-threaded manner for capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce a user interaction for establishing a global coordinate system and for locating virtual objects in the {AR} environment. A user's bare hand is used for the user interface by estimating a camera pose relative to the user's outstretched hand. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling {AR} in unprepared tabletop environments using hands for interaction.},
	booktitle = {{IEEE} Virtual Reality Conference, 2008. {VR} '08},
	author = {Lee, Taehee and Hollerer, T.},
	month = mar,
	year = {2008},
	keywords = {augmented reality, Cameras, Computer architecture, Computer vision, distinctive image features, hybrid feature tracking, I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism??Virtual Reality, I.4.8 [Image Processing and Computer Vision]: Scene Analysis, image motion analysis, image sequences, interaction techniques for {MR}/{AR}, Layout, markerless augmented reality, markerless camera tracking, multi-threading, multithreading, object detection, Optical computing, Optical detectors, optical flow, optical tracking, position and orientation tracking technology, real-time system architecture, Real time systems, real-time systems, rendering, rendering (computer graphics), scene detection, tabletop environment, user interaction, user interfaces, video frame, video signal processing, virtual objects, vision-based registration and tracking},
	pages = {145--152},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\CM7UD4QR\\articleDetails.html:text/html;Lee 그리고 Hollerer - 2008 - Hybrid Feature Tracking and User Interaction for M.pdf:E\:\\Papers\\Zotero Repository\\storage\\S9G7MSJJ\\Lee 그리고 Hollerer - 2008 - Hybrid Feature Tracking and User Interaction for M.pdf:application/pdf}
}

@article{lepetit_keypoint_2006,
	title = {Keypoint recognition using randomized trees},
	volume = {28},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2006.188},
	abstract = {In many 3D object-detection and pose-estimation problems, runtime performance is of critical importance. However, there usually is time to train the system, which we would show to be very useful. Assuming that several registered images of the target object are available, we developed a keypoint-based approach that is effective in this context by formulating wide-baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem. This shifts much of the computational burden to a training phase, without sacrificing recognition performance. As a result, the resulting algorithm is robust, accurate, and fast-enough for frame-rate performance. This reduction in runtime computational complexity is our first contribution. Our second contribution is to show that, in this context, a simple and fast keypoint detector suffices to support detection and tracking even under large perspective and scale variations. While earlier methods require a detector that can be expected to produce very repeatable results, in general, which usually is very time-consuming, we simply find the most repeatable object keypoints for the specific target object during the training phase. We have incorporated these ideas into a real-time system that detects planar, nonplanar, and deformable objects. It then estimates the pose of the rigid ones and the deformations of the others},
	number = {9},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lepetit, V. and Fua, P.},
	month = sep,
	year = {2006},
	keywords = {3D object-detection, Algorithms, Artificial Intelligence, Classification tree analysis, classifier design and evaluation, computational complexity, Computer Simulation, Computer vision, Data Interpretation, Statistical, Deformable models, edge and feature detection., feature extraction, image classification, Image Enhancement, Image Interpretation, Computer-Assisted, Image Processing and Computer Vision, Imaging, Three-Dimensional, Information Storage and Retrieval, keypoint recognition, Models, Statistical, object detection, object recognition, Pattern Recognition, Pattern Recognition, Automated, Phase detection, Phase estimation, pose-estimation problem, Processor scheduling, randomized trees, Robustness, Runtime, statistical, Statistical learning, tracking, trees (mathematics)},
	pages = {1465--1479},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\I3DFRT72\\articleDetails.html:text/html;Lepetit 그리고 Fua - 2006 - Keypoint recognition using randomized trees.pdf:E\:\\Papers\\Zotero Repository\\storage\\ES626QMX\\Lepetit 그리고 Fua - 2006 - Keypoint recognition using randomized trees.pdf:application/pdf}
}

@inproceedings{nister_visual_2004,
	title = {Visual odometry},
	volume = {1},
	doi = {10.1109/CVPR.2004.1315094},
	abstract = {We present a system that estimates the motion of a stereo head or a single moving camera based on video input. The system operates in real-time with low delay and the motion estimates are used for navigational purposes. The front end of the system is a feature tracker. Point features are matched between pairs of frames and linked into image trajectories at video rate. Robust estimates of the camera motion are then produced from the feature tracks using a geometric hypothesize-and-test architecture. This generates what we call visual odometry, i.e. motion estimates from visual input alone. No prior knowledge of the scene nor the motion is necessary. The visual odometry can also be used in conjunction with information from other sources such as {GPS}, inertia sensors, wheel encoders, etc. The pose estimation method has been applied successfully to video from aerial, automotive and handheld platforms. We focus on results with an autonomous ground vehicle. We give examples of camera trajectories estimated purely from images over previously unseen distances and periods of time.},
	booktitle = {Proceedings of the 2004 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 2004. {CVPR} 2004},
	author = {Nister, D. and Naroditsky, O. and Bergen, J.},
	month = jun,
	year = {2004},
	keywords = {aerial platforms, automotive platforms, autonomous ground vehicle, camera motion, Cameras, camera trajectories, Delay estimation, feature tracker, geometric hypothesize-and-test architecture, Global Positioning System, handheld platforms, Head, image matching, image trajectories, inertial navigation, Layout, motion estimation, Navigation, pose estimation method, Real time systems, robust estimation, Robustness, single moving camera, stereo head, tracking, vehicles, video cameras, video rate, video signal processing, visual odometry},
	pages = {I--652--I--659 Vol.1},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\2UTEWVSV\\articleDetails.html:text/html;Nister et al. - 2004 - Visual odometry.pdf:E\:\\Papers\\Zotero Repository\\storage\\KPKPQPJW\\Nister et al. - 2004 - Visual odometry.pdf:application/pdf}
}

@inproceedings{ozuysal_fast_2007,
	title = {Fast Keypoint Recognition in Ten Lines of Code},
	doi = {10.1109/CVPR.2007.383123},
	abstract = {While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a Naive Bayesian classification framework makes such preprocessing unnecessary and produces an algorithm that is simple, efficient, and robust. Furthermore, it scales well to handle large number of classes. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and models class posterior probabilities. We make the problem computationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless performs remarkably well on image datasets containing very significant perspective changes.},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, 2007. {CVPR} '07},
	author = {Ozuysal, M. and Fua, P. and Lepetit, V.},
	month = jun,
	year = {2007},
	keywords = {Bayesian methods, Casting, class posterior probabilities, code, Computer vision, image classification, image datasets, Image recognition, image registration, Image retrieval, keypoint recognition, Laboratories, naive Bayesian classification framework, object detection, patch preprocessing, perspective distortion, probability, Robustness, ten lines, Testing},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\BU58NC7K\\articleDetails.html:text/html;Ozuysal et al. - 2007 - Fast Keypoint Recognition in Ten Lines of Code.pdf:E\:\\Papers\\Zotero Repository\\storage\\PM5I8FSM\\Ozuysal et al. - 2007 - Fast Keypoint Recognition in Ten Lines of Code.pdf:application/pdf}
}

@inproceedings{park_multiple_2008,
	title = {Multiple 3D Object tracking for augmented reality},
	doi = {10.1109/ISMAR.2008.4637336},
	abstract = {We present a method that is able to track several 3D objects simultaneously, robustly, and accurately in real-time. While many applications need to consider more than one object in practice, the existing methods for single object tracking do not scale well with the number of objects, and a proper way to deal with several objects is required. Our method combines object detection and tracking: Frame-to-frame tracking is less computationally demanding but is prone to fail, while detection is more robust but slower. We show how to combine them to take the advantages of the two approaches, and demonstrate our method on several real sequences.},
	booktitle = {7th {IEEE}/{ACM} International Symposium on Mixed and Augmented Reality, 2008. {ISMAR} 2008},
	author = {Park, Youngmin and Lepetit, V. and Woo, Woontack},
	month = sep,
	year = {2008},
	keywords = {Application software, augmented reality, Computer vision, feature extraction, frame-to-frame tracking, Image recognition, Impedance matching, multiple 3D object tracking, object detection, Robustness, simultaneous localization and mapping, target tracking, tracking},
	pages = {117--120},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\3EW7DNQT\\articleDetails.html:text/html;Park et al. - 2008 - Multiple 3D Object tracking for augmented reality.pdf:E\:\\Papers\\Zotero Repository\\storage\\SA5QXIVC\\Park et al. - 2008 - Multiple 3D Object tracking for augmented reality.pdf:application/pdf}
}

@article{se_mobile_2002,
	title = {Mobile Robot Localization and Mapping with Uncertainty using Scale-Invariant Visual Landmarks},
	volume = {21},
	issn = {0278-3649, 1741-3176},
	url = {http://ijr.sagepub.com/content/21/8/735},
	doi = {10.1177/027836402761412467},
	abstract = {A key component of a mobile robot system is the ability to localize itself accurately and, simultaneously, to build a map of the environment. Most of the existing algorithms are based on laser range finders, sonar sensors or artificial landmarks. In this paper, we describe a vision-based mobile robot localization and mapping algorithm, which uses scale-invariant image features as natural landmarks in unmodified environments. The invariance of these features to image translation, scaling and rotation makes them suitable landmarks for mobile robot localization and map building. With our Triclops stereo vision system, these landmarks are localized and robot ego-motion is estimated by least-squares minimization of the matched landmarks. Feature viewpoint variation and occlusion are taken into account by maintaining a view direction for each landmark. Experiments show that these visual landmarks are robustly matched, robot pose is estimated and a consistent three-dimensional map is built. As image features are not noise-free, we carry out error analysis for the landmark positions and the robot pose. We use Kalman filters to track these landmarks in a dynamic environment, resulting in a database map with landmark positional uncertainty.},
	language = {en},
	number = {8},
	urldate = {2014-09-11},
	journal = {The International Journal of Robotics Research},
	author = {Se, Stephen and Lowe, David and Little, Jim},
	month = aug,
	year = {2002},
	keywords = {localization, mapping, mobile robot, visual landmarks},
	pages = {735--758},
	file = {Se et al. - 2002 - Mobile Robot Localization and Mapping with Uncerta.pdf:E\:\\Papers\\Zotero Repository\\storage\\JIHJSJG5\\Se et al. - 2002 - Mobile Robot Localization and Mapping with Uncerta.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\TDHBFPRZ\\735.html:text/html}
}

@inproceedings{skrypnyk_scene_2004,
	title = {Scene modelling, recognition and tracking with invariant image features},
	doi = {10.1109/ISMAR.2004.53},
	abstract = {We present a complete system architecture for fully automated markerless augmented reality ({AR}). The system constructs a sparse metric model of the real-world environment, provides interactive means for specifying the pose of a virtual object, and performs model-based camera tracking with visually pleasing augmentation results. Our approach does not require camera pre-calibration, prior knowledge of scene geometry, manual initialization of the tracker or placement of special markers. Robust tracking in the presence of occlusions and scene changes is achieved by using highly distinctive natural features to establish image correspondences.},
	booktitle = {Third {IEEE} and {ACM} International Symposium on Mixed and Augmented Reality, 2004. {ISMAR} 2004},
	author = {Skrypnyk, I and Lowe, D.G.},
	month = nov,
	year = {2004},
	keywords = {augmented reality, automated markerless augmented reality, Calibration, Cameras, Computer architecture, Computer science, Geometry, image correspondences, Image recognition, invariant image features, Layout, model-based camera tracking, natural features, occlusions, optical tracking, Power system modeling, real-world environment, Robustness, robust tracking, scene changes, scene modelling, scene recognition, scene tracking, solid modelling, sparse metric model, system architecture, virtual object},
	pages = {110--119},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\UKSVPK4G\\articleDetails.html:text/html;Skrypnyk 그리고 Lowe - 2004 - Scene modelling, recognition and tracking with inv.pdf:E\:\\Papers\\Zotero Repository\\storage\\B9F7J2AS\\Skrypnyk 그리고 Lowe - 2004 - Scene modelling, recognition and tracking with inv.pdf:application/pdf}
}

@inproceedings{taylor_robust_2009,
	title = {Robust feature matching in 2.3 \#x00B5;s},
	doi = {10.1109/CVPRW.2009.5204314},
	abstract = {In this paper we present a robust feature matching scheme in which features can be matched in 2.3 mus. For a typical task involving 150 features per image, this results in a processing time of 500 mus for feature extraction and matching. In order to achieve very fast matching we use simple features based on histograms of pixel intensities and an indexing scheme based on their joint distribution. The features are stored with a novel bit mask representation which requires only 44 bytes of memory per feature and allows computation of a dissimilarity score in 20 ns. A training phase gives the patch-based features invariance to small viewpoint variations. Larger viewpoint variations are handled by training entirely independent sets of features from different viewpoints. A complete system is presented where a database of around 13,000 features is used to robustly localise a single planar target in just over a millisecond, including all steps from feature detection to model fitting. The resulting system shows comparable robustness to {SIFT} and Ferns while using a tiny fraction of the processing time, and in the latter case a fraction of the memory as well.},
	booktitle = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2009. {CVPR} Workshops 2009},
	author = {Taylor, S. and Rosten, Edward and Drummond, Tom},
	month = jun,
	year = {2009},
	keywords = {bit mask representation, Computer vision, database indexing, Data mining, feature detection, feature extraction, Histograms, Image databases, Image edge detection, image matching, image representation, indexing, indexing scheme, model fitting, robust feature matching scheme, Robustness, Runtime, Spatial databases, visual databases},
	pages = {15--22},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\6IHUVVR2\\articleDetails.html:text/html;Taylor et al. - 2009 - Robust feature matching in 2.3 #x00B5\;s.pdf:E\:\\Papers\\Zotero Repository\\storage\\VF5TDV9K\\Taylor et al. - 2009 - Robust feature matching in 2.3 #x00B5\;s.pdf:application/pdf}
}

@inproceedings{wagner_multiple_2009,
	title = {Multiple target detection and tracking with guaranteed framerates on mobile phones},
	doi = {10.1109/ISMAR.2009.5336497},
	abstract = {In this paper we present a novel method for real-time pose estimation and tracking on low-end devices such as mobile phones. The presented system can track multiple known targets in real-time and simultaneously detect new targets for tracking. We present a method to automatically and dynamically balance the quality of detection and tracking to adapt to a variable time budget and ensure a constant frame rate. Results from real data of a mobile phone Augmented Reality system demonstrate the efficiency and robustness of the described approach. The system can track 6 planar targets on a mobile phone simultaneously at framerates of 23 fps.},
	booktitle = {8th {IEEE} International Symposium on Mixed and Augmented Reality, 2009. {ISMAR} 2009},
	author = {Wagner, Daniel and Schmalstieg, D. and Bischof, H.},
	month = oct,
	year = {2009},
	keywords = {6DOF, augmented reality, augmented reality system, Cameras, constant frame rate, mobile computing, mobile handsets, mobile phone, multiple target detection, natural features, object detection, Optical filters, Optical mixing, Optical sensors, pose estimation, real-time pose estimation, Real time systems, Robustness, target tracking, tracking},
	pages = {57--64},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\ANDMG6PR\\articleDetails.html:text/html;Wagner et al. - 2009 - Multiple target detection and tracking with guaran.pdf:E\:\\Papers\\Zotero Repository\\storage\\9BF326V3\\Wagner et al. - 2009 - Multiple target detection and tracking with guaran.pdf:application/pdf}
}

@inproceedings{wagner_real-time_2010,
	title = {Real-time panoramic mapping and tracking on mobile phones},
	doi = {10.1109/VR.2010.5444786},
	abstract = {We present a novel method for the real-time creation and tracking of panoramic maps on mobile phones. The maps generated with this technique are visually appealing, very accurate and allow drift-free rotation tracking. This method runs on mobile phones at 30 Hz and has applications in the creation of panoramic images for offline browsing, for visual enhancements through environment mapping and for outdoor Augmented Reality on mobile phones.},
	booktitle = {2010 {IEEE} Virtual Reality Conference ({VR})},
	author = {Wagner, Daniel and Mulloni, Alessandro and Langlotz, Tobias and Schmalstieg, D.},
	month = mar,
	year = {2010},
	keywords = {Accelerometers, Application software, augmented reality, Cameras, Computer vision, drift-free rotation tracking, Image Enhancement, Intelligent sensors, mobile handsets, mobile phone, mobile phones, offline browsing, outdoor augmented reality, Panorama creation, panoramic images, real-time panoramic mapping, real-time systems, real-time tracking, Robustness, simultaneous localization and mapping, tracking, virtual reality, visual enhancements},
	pages = {211--218},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\9RFXXWHM\\abs_all.html:text/html;Wagner et al. - 2010 - Real-time panoramic mapping and tracking on mobile.pdf:E\:\\Papers\\Zotero Repository\\storage\\I9E2UR8X\\Wagner et al. - 2010 - Real-time panoramic mapping and tracking on mobile.pdf:application/pdf}
}

@inproceedings{williams_real-time_2007,
	title = {Real-Time {SLAM} Relocalisation},
	doi = {10.1109/ICCV.2007.4409115},
	abstract = {Monocular {SLAM} has the potential to turn inexpensive cameras into powerful pose sensors for applications such as robotics and augmented reality. However, current implementations lack the robustness required to be useful outside laboratory conditions: blur, sudden motion and occlusion all cause tracking to fail and corrupt the map. Here we present a system which automatically detects and recovers from tracking failure while preserving map integrity. By extending recent advances in keypoint recognition the system can quickly resume tracking - i.e. within a single frame time of 33 ms - using any of the features previously stored in the map. Extensive tests show that the system can reliably generate maps for long sequences even in the presence of frequent tracking failure.},
	booktitle = {{IEEE} 11th International Conference on Computer Vision, 2007. {ICCV} 2007},
	author = {Williams, B. and Klein, G. and Reid, I},
	month = oct,
	year = {2007},
	keywords = {augmented reality, automatic tracking failure detection, automatic tracking failure recovery, Cameras, Image recognition, image restoration, image sequence, image sequences, Laboratories, mobile robots, optical tracking, real-time monocular {SLAM} relocalisation, Resumes, Robotics and automation, Robot sensing systems, robot vision, Robot vision systems, Robustness, simultaneous localization and mapping, {SLAM} (robots), tracking},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\4NR3WAHK\\articleDetails.html:text/html;Williams et al. - 2007 - Real-Time SLAM Relocalisation.pdf:E\:\\Papers\\Zotero Repository\\storage\\JVWZBHCM\\Williams et al. - 2007 - Real-Time SLAM Relocalisation.pdf:application/pdf}
}

@inproceedings{chen_robust_2013,
	title = {Robust Feature Matching with Alternate Hough and Inverted Hough Transforms},
	doi = {10.1109/CVPR.2013.356},
	abstract = {We present an algorithm that carries out alternate Hough transform and inverted Hough transform to establish feature correspondences, and enhances the quality of matching in both precision and recall. Inspired by the fact that nearby features on the same object share coherent homographies in matching, we cast the task of feature matching as a density estimation problem in the Hough space spanned by the hypotheses of homographies. Specifically, we project all the correspondences into the Hough space, and determine the correctness of the correspondences by their respective densities. In this way, mutual verification of relevant correspondences is activated, and the precision of matching is boosted. On the other hand, we infer the concerted homographies propagated from the locally grouped features, and enrich the correspondence candidates for each feature. The recall is hence increased. The two processes are tightly coupled. Through iterative optimization, plausible enrichments are gradually revealed while more correct correspondences are detected. Promising experimental results on three benchmark datasets manifest the effectiveness of the proposed approach.},
	booktitle = {2013 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Chen, Hsin-Yi and Lin, Yen-Yu and Chen, Bing-Yu},
	year = {2013},
	keywords = {alternate Hough transform, coherent homography, concerted homography, correspondence enrichment, density estimation problem, Detectors, Estimation, estimation theory, feature extraction, Hough space, Hough transform, Hough transforms, image matching, inverted Hough transform, {IP} networks, iterative methods, iterative optimization, Lighting, optimisation, plausible enrichments, point matching, robust feature matching, Robustness, Transforms},
	pages = {2762--2769},
	file = {Chen et al. - 2013 - Robust Feature Matching with Alternate Hough and I.pdf:E\:\\Papers\\Zotero Repository\\storage\\VXUF65C3\\Chen et al. - 2013 - Robust Feature Matching with Alternate Hough and I.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\HZUUHWNJ\\abs_all.html:text/html}
}

@inproceedings{csurka_visual_2004,
	title = {Visual categorization with bags of keypoints},
	abstract = {Abstract. We present a novel method for generic visual categorization: the problem of identifying the object content of natural images while generalizing across variations inherent to the object class. This bag of keypoints method is based on vector quantization of affine invariant descriptors of image patches. We propose and compare two alternative implementations using different classifiers: Naïve Bayes and {SVM}. The main advantages of the method are that it is simple, computationally efficient and intrinsically invariant. We present results for simultaneously classifying seven semantic visual categories. These results clearly demonstrate that the method is robust to background clutter and produces good categorization accuracy even without exploiting geometric information. 1.},
	booktitle = {In Workshop on Statistical Learning in Computer Vision, {ECCV}},
	author = {Csurka, Gabriella and Dance, Christopher R. and Fan, Lixin and Willamowski, Jutta and Bray, Cédric},
	year = {2004},
	pages = {1--22},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\G83RIEUP\\Csurka et al. - 2004 - Visual categorization with bags of keypoints.pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\NMUP59PW\\summary.html:text/html}
}

@inproceedings{barandiaran_comparative_2008,
	title = {Comparative Evaluation of Random Forest and Fern  Classifiers for Real-Time Feature Matching},
	isbn = {978-80-86943-15-2},
	abstract = {Feature or keypoint matching is a critical task in many computer vision applications, such as optical 3D 
reconstruction or optical markerless tracking. These applications demand very accurate and fast matching 
techniques. We present an evaluation and comparison of two keypoint matching strategies based on supervised 
classification for markerless tracking of planar surfaces. We have applied these approaches on an augmented 
reality prototype for indoor and outdoor design review.},
	booktitle = {Proceedings of {WSCG}},
	author = {Barandiaran, Iñigo and Cottez, Charlote and Paloc, Céline and Graña, Manuel},
	year = {2008},
	pages = {59--166},
	file = {F71-full.pdf:E\:\\Papers\\Zotero Repository\\storage\\M9EFAV2C\\F71-full.pdf:application/pdf}
}

@incollection{lepetit_keypoint_2013,
	series = {Advances in Computer Vision and Pattern Recognition},
	title = {Keypoint Recognition Using Random Forests and Random Ferns},
	copyright = {©2013 Springer-Verlag London},
	isbn = {978-1-4471-4928-6, 978-1-4471-4929-3},
	url = {http://link.springer.com/chapter/10.1007/978-1-4471-4929-3_9},
	abstract = {In many 3D object detection and pose estimation problems, run-time performance is of critical importance. However, there usually is time to train the system. We introduce an approach that takes advantage of this fact by formulating the wide-baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem. This shifts much of the computational burden to a training phase and eliminates the need for expensive patch preprocessing, without sacrificing recognition performance. This makes our approach highly suitable for real-time operations on low-powered devices. To this end, we developed two related methods. The first uses random forests that rely on simple binary tests on image intensities surrounding the keypoints. In the second, we flatten the trees to turn them into simple bit strings, which we will refer to as ferns, and combine their output in a Naïve Bayesian manner. Surprisingly, the ferns, while simpler, actually perform better than the trees. This is because the Naïve Bayesian approach benefits more from the thousands of synthetic training examples we can generate than output averaging as usually performed by decision forests. Furthermore, the more general partition that the trees allow does not appear to be of great use for our problem. Parts of this chapter are reprinted, with permission, from Lepetit, Lagger, and Fua, Proc. {IEEE} Conf. Computer Vision and Pattern Recognition ({CVPR}) (2005), © 2005 {IEEE}.},
	language = {en},
	urldate = {2014-02-03},
	booktitle = {Decision Forests for Computer Vision and Medical Image Analysis},
	publisher = {Springer London},
	author = {Lepetit, V. and Fua, P.},
	editor = {Criminisi, A. and Shotton, J.},
	month = jan,
	year = {2013},
	keywords = {Artificial Intelligence (incl. Robotics), Pattern Recognition},
	pages = {111--124},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\BU8CWFEC\\Lepetit 그리고 Fua - 2013 - Keypoint Recognition Using Random Forests and Rand.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\HFTZKGX3\\978-1-4471-4929-3_9.html:text/html}
}

@article{ozuysal_fast_2010,
	title = {Fast Keypoint Recognition Using Random Ferns},
	volume = {32},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2009.23},
	abstract = {While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a naive Bayesian classification framework makes such preprocessing unnecessary and produces an algorithm that is simple, efficient, and robust. Furthermore, it scales well as the number of classes grows. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and models class posterior probabilities. We make the problem computationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless performs remarkably well on image data sets containing very significant perspective changes.},
	number = {3},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ozuysal, M. and Calonder, M. and Lepetit, V. and Fua, P.},
	month = mar,
	year = {2010},
	keywords = {Bayes methods, classifier, Computer vision, fast keypoint recognition, feature matching, feature point recognition, image classification, image data sets, Image Processing and Computer Vision, image registration, naive Bayesian., naive Bayesian classification framework, object detection, object recognition, posterior probabilities, probability, random ferns, tracking},
	pages = {448--461},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\KSSAPIZ9\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\SDRBCQMR\\Ozuysal et al. - 2010 - Fast Keypoint Recognition Using Random Ferns.pdf:application/pdf}
}

@inproceedings{muja_fast_2012,
	title = {Fast Matching of Binary Features},
	doi = {10.1109/CRV.2012.60},
	abstract = {There has been growing interest in the use of binary-valued features, such as {BRIEF}, {ORB}, and {BRISK} for efficient local feature matching. These binary features have several advantages over vector-based features as they can be faster to compute, more compact to store, and more efficient to compare. Although it is fast to compute the Hamming distance between pairs of binary features, particularly on modern architectures, it can still be too slow to use linear search in the case of large datasets. For vector-based features, such as {SIFT} and {SURF}, the solution has been to use approximate nearest-neighbor search, but these existing algorithms are not suitable for binary features. In this paper we introduce a new algorithm for approximate matching of binary features, based on priority search of multiple hierarchical clustering trees. We compare this to existing alternatives, and show that it performs well for large datasets, both in terms of speed and memory efficiency.},
	booktitle = {2012 Ninth Conference on Computer and Robot Vision ({CRV})},
	author = {Muja, M. and Lowe, D.G.},
	month = may,
	year = {2012},
	keywords = {approximate matching, Approximation algorithms, binary features, binary-valued features, {BRIEF}, {BRISK}, Buildings, Clustering algorithms, fast matching, feature extraction, feature matching, Hamming distance, hierarchical clustering trees, image matching, large datasets, Libraries, linear search, local feature matching, memory efficiency, nearest neighbors, nearest-neighbor search, Nearest neighbor searches, {ORB}, pattern clustering, search problems, {SIFT}, speed efficiency, {SURF}, trees (mathematics), vector-based features, Vectors},
	pages = {404--410},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\MFEX5JJ9\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\78FQUVH9\\Muja 그리고 Lowe - 2012 - Fast Matching of Binary Features.pdf:application/pdf}
}

@inproceedings{choi_smart_2014,
	title = {Smart Booklet: Tour guide system with mobile augmented reality},
	shorttitle = {Smart Booklet},
	doi = {10.1109/ICCE.2014.6776038},
	abstract = {This paper presents a mobile tour guide system with augmented reality, called Smart Booklet. The system enables tourists to have more informative, interactive and user-specific experiences with augmented information by recognizing/tracking the contents of an off-line tour booklet. For a stand-alone mobile system, demanding low computational cost, an image matching technique based on the combination of two binary feature descriptors are applied. Our proposed system can be applicable to many areas such as education and entertainment industries.},
	booktitle = {2014 {IEEE} International Conference on Consumer Electronics ({ICCE})},
	author = {Choi, Heeseung and Han, Gyu Chull and Kim, Ig-jae},
	month = jan,
	year = {2014},
	keywords = {augmented information, augmented reality, binary feature descriptors, computational cost, Databases, feature extraction, image matching, image matching technique, Image recognition, informative-interactive-user-specific experiences, mobile augmented reality, Mobile communication, mobile computing, mobile tour guide system, offline tour booklet content recognition, offline tour booklet content tracking, Robustness, Smart Booklet, Solid modeling, stand-alone mobile system, target tracking, travel industry},
	pages = {353--354},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\WWV3N3MG\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\CC5BSN6H\\Choi et al. - 2014 - Smart Booklet Tour guide system with mobile augme.pdf:application/pdf}
}

@inproceedings{beis_shape_1997,
	title = {Shape indexing using approximate nearest-neighbour search in high-dimensional spaces},
	doi = {10.1109/CVPR.1997.609451},
	abstract = {Shape indexing is a way of making rapid associations between features detected in an image and object models that could have produced them. When model databases are large, the use of high-dimensional features is critical, due to the improved level of discrimination they can provide. Unfortunately, finding the nearest neighbour to a query point rapidly becomes inefficient as the dimensionality of the feature space increases. Past indexing methods have used hash tables for hypothesis recovery, but only in low-dimensional situations. In this paper we show that a new variant of the k-d tree search algorithm makes indexing in higher-dimensional spaces practical. This Best Bin First, or {BBF} search is an approximate algorithm which finds the nearest neighbour for a large fraction of the queries, and a very close neighbour in the remaining cases. The technique has been integrated into a fully developed recognition system, which is able to detect complex objects in real, cluttered scenes in just a few seconds},
	booktitle = {, 1997 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition, 1997. Proceedings},
	author = {Beis, J.S. and Lowe, D.G.},
	month = jun,
	year = {1997},
	keywords = {approximate algorithm, approximate nearest-neighbour search, Computer science, Computer vision, feature extraction, features detection, hash tables, high-dimensional spaces, hypothesis recovery, Image databases, image models, indexing, indexing methods, k-d tree search algorithm, Layout, model databases, nearest neighbour, Neural networks, object detection, object models, query point, Shape, shape indexing, Spatial databases, tree data structures, tree searching, visual databases},
	pages = {1000--1006},
	file = {Beis 그리고 Lowe - 1997 - Shape indexing using approximate nearest-neighbour.pdf:E\:\\Papers\\Zotero Repository\\storage\\NHZK3XVC\\Beis 그리고 Lowe - 1997 - Shape indexing using approximate nearest-neighbour.pdf:application/pdf;IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\TPWPI2PA\\abs_all.html:text/html}
}

@article{lepetit_keypoint_2006-1,
	title = {Keypoint recognition using randomized trees},
	volume = {28},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2006.188},
	abstract = {In many 3D object-detection and pose-estimation problems, runtime performance is of critical importance. However, there usually is time to train the system, which we would show to be very useful. Assuming that several registered images of the target object are available, we developed a keypoint-based approach that is effective in this context by formulating wide-baseline matching of keypoints extracted from the input images to those found in the model images as a classification problem. This shifts much of the computational burden to a training phase, without sacrificing recognition performance. As a result, the resulting algorithm is robust, accurate, and fast-enough for frame-rate performance. This reduction in runtime computational complexity is our first contribution. Our second contribution is to show that, in this context, a simple and fast keypoint detector suffices to support detection and tracking even under large perspective and scale variations. While earlier methods require a detector that can be expected to produce very repeatable results, in general, which usually is very time-consuming, we simply find the most repeatable object keypoints for the specific target object during the training phase. We have incorporated these ideas into a real-time system that detects planar, nonplanar, and deformable objects. It then estimates the pose of the rigid ones and the deformations of the others},
	number = {9},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Lepetit, V. and Fua, P.},
	month = sep,
	year = {2006},
	keywords = {3D object-detection, Classification tree analysis, classifier design and evaluation, computational complexity, Computer vision, Deformable models, edge and feature detection., feature extraction, image classification, Image Processing and Computer Vision, keypoint recognition, object detection, object recognition, Pattern Recognition, Phase detection, Phase estimation, pose-estimation problem, Processor scheduling, randomized trees, Robustness, Runtime, statistical, Statistical learning, tracking, trees (mathematics)},
	pages = {1465--1479},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\NRRSTFCW\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\XH9F9NDX\\Lepetit 그리고 Fua - 2006 - Keypoint recognition using randomized trees.pdf:application/pdf}
}

@inproceedings{gionis_similarity_1999,
	address = {San Francisco, {CA}, {USA}},
	series = {{VLDB} '99},
	title = {Similarity Search in High Dimensions via Hashing},
	isbn = {1-55860-615-7},
	url = {http://dl.acm.org/citation.cfm?id=645925.671516},
	urldate = {2014-07-19},
	booktitle = {Proceedings of the 25th International Conference on Very Large Data Bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
	year = {1999},
	pages = {518--529},
	file = {Gionis et al. - 1999 - Similarity Search in High Dimensions via Hashing.pdf:E\:\\Papers\\Zotero Repository\\storage\\W6I9RE9V\\Gionis et al. - 1999 - Similarity Search in High Dimensions via Hashing.pdf:application/pdf;Hashing.ppt:E\:\\Papers\\Zotero Repository\\storage\\66IF5B8Q\\Hashing.ppt:application/msword}
}

@inproceedings{andoni_near-optimal_2006,
	title = {Near-Optimal Hashing Algorithms for Approximate Nearest Neighbor in High Dimensions},
	doi = {10.1109/FOCS.2006.49},
	abstract = {We present an algorithm for the c-approximate nearest neighbor problem in a d-dimensional Euclidean space, achieving query time of O(dn 1c2/+o(1)) and space O(dn + n1+1c2/+o(1)). This almost matches the lower bound for hashing-based algorithm recently obtained in (R. Motwani et al., 2006). We also obtain a space-efficient version of the algorithm, which uses dn+n {logO}(1) n space, with a query time of {dnO}(1/c2). Finally, we discuss practical variants of the algorithms that utilize fast bounded-distance decoders for the Leech lattice},
	booktitle = {47th Annual {IEEE} Symposium on Foundations of Computer Science, 2006. {FOCS} '06},
	author = {Andoni, A and Indyk, P.},
	month = oct,
	year = {2006},
	keywords = {Approximation algorithms, bounded-distance decoder, c-approximate nearest neighbor, computational complexity, data compression, Data mining, Data structures, d-dimensional Euclidean space, Decoding, file organisation, Image databases, Lattices, Leech lattice, Nearest neighbor searches, near-optimal hashing algorithms, Probes, Spatial databases},
	pages = {459--468},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\D6P7V2QS\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\TJ4UKVTK\\Andoni 그리고 Indyk - 2006 - Near-Optimal Hashing Algorithms for Approximate Ne.pdf:application/pdf}
}

@article{muja_scalable_2014,
	title = {Scalable Nearest Neighbor Algorithms for High Dimensional Data},
	volume = {36},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2014.2321376},
	abstract = {For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors ({FLANN}), which has been incorporated into {OpenCV} and is now one of the most popular libraries for nearest neighbor matching.},
	number = {11},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Muja, M. and Lowe, D.G.},
	month = nov,
	year = {2014},
	keywords = {algorithm configuration, approximate search, Approximation algorithms, Approximation methods, automated configuration procedure, big data, Clustering algorithms, Computer vision, distributed nearest neighbor matching framework, fast library for approximate nearest neighbors, {FLANN}, high dimensional data, high dimensional feature matching, high dimensional vectors, image matching, learning (artificial intelligence), Machine learning algorithms, machine learning problems, multiple hierarchical clustering trees, Nearest neighbor search, open source library, Partitioning algorithms, priority search k-means tree, randomized k-d forest algorithm, scalable nearest neighbor algorithms, search problems, trees (mathematics), Vegetation},
	pages = {2227--2240},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\7SSANUTZ\\abs_all.html:text/html;Muja 그리고 Lowe - 2014 - Scalable Nearest Neighbor Algorithms for High Dime.pdf:E\:\\Papers\\Zotero Repository\\storage\\ECE72WTF\\Muja 그리고 Lowe - 2014 - Scalable Nearest Neighbor Algorithms for High Dime.pdf:application/pdf}
}

@article{pauleve_locality_2010,
	title = {Locality sensitive hashing: A comparison of hash function types and querying mechanisms},
	volume = {31},
	issn = {0167-8655},
	shorttitle = {Locality sensitive hashing},
	url = {http://www.sciencedirect.com/science/article/pii/S0167865510001169},
	doi = {10.1016/j.patrec.2010.04.004},
	abstract = {It is well known that high-dimensional nearest neighbor retrieval is very expensive. Dramatic performance gains are obtained using approximate search schemes, such as the popular Locality-Sensitive Hashing ({LSH}). Several extensions have been proposed to address the limitations of this algorithm, in particular, by choosing more appropriate hash functions to better partition the vector space. All the proposed extensions, however, rely on a structured quantizer for hashing, poorly fitting real data sets, limiting its performance in practice. In this paper, we compare several families of space hashing functions in a real setup, namely when searching for high-dimension {SIFT} descriptors. The comparison of random projections, lattice quantizers, k-means and hierarchical k-means reveal that unstructured quantizer significantly improves the accuracy of {LSH}, as it closely fits the data in the feature space. We then compare two querying mechanisms introduced in the literature with the one originally proposed in {LSH}, and discuss their respective merits and limitations.},
	number = {11},
	urldate = {2014-11-08},
	journal = {Pattern Recognition Letters},
	author = {Paulevé, Loïc and Jégou, Hervé and Amsaleg, Laurent},
	month = aug,
	year = {2010},
	keywords = {Database searching, Image databases, Information retrieval, {LSH}, Quantization, Search methods},
	pages = {1348--1358},
	file = {PJA10.pdf:E\:\\Papers\\Zotero Repository\\storage\\348WNZK7\\PJA10.pdf:application/pdf;ScienceDirect Snapshot:E\:\\Papers\\Zotero Repository\\storage\\SR3GST9Q\\S0167865510001169.html:text/html}
}

@inproceedings{nister_scalable_2006-1,
	title = {Scalable Recognition with a Vocabulary Tree},
	volume = {2},
	doi = {10.1109/CVPR.2006.264},
	abstract = {A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes {CD}-covers from a database of 40000 images of popular music {CD}’s. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.},
	booktitle = {2006 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition},
	author = {Nister, D. and Stewenius, H.},
	year = {2006},
	keywords = {Computer vision, Frequency, Image databases, Image recognition, indexing, Quantization, Robustness, Spatial databases, Visualization, vocabulary},
	pages = {2161--2168},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\SU2GW38F\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\6N6AHFKV\\Nister 그리고 Stewenius - 2006 - Scalable Recognition with a Vocabulary Tree.pdf:application/pdf}
}

@inproceedings{saffari_-line_2009,
	title = {On-line Random Forests},
	doi = {10.1109/ICCVW.2009.5457447},
	abstract = {Random Forests ({RFs}) are frequently used in many computer vision and machine learning applications. Their popularity is mainly driven by their high computational efficiency during both training and evaluation while achieving state-of-the-art results. However, in most applications {RFs} are used off-line. This limits their usability for many practical problems, for instance, when training data arrives sequentially or the underlying distribution is continuously changing. In this paper, we propose a novel on-line random forest algorithm. We combine ideas from on-line bagging, extremely randomized forests and propose an on-line decision tree growing procedure. Additionally, we add a temporal weighting scheme for adaptively discarding some trees based on their out-of-bag-error in given time intervals and consequently growing of new trees. The experiments on common machine learning data sets show that our algorithm converges to the performance of the off-line {RF}. Additionally, we conduct experiments for visual tracking, where we demonstrate real-time state-of-the-art performance on well-known scenarios and show good performance in case of occlusions and appearance changes where we outperform trackers based on on-line boosting. Finally, we demonstrate the usability of on-line {RFs} on the task of interactive real-time segmentation.},
	booktitle = {2009 {IEEE} 12th International Conference on Computer Vision Workshops ({ICCV} Workshops)},
	author = {Saffari, A. and Leistner, C. and Santner, J. and Godec, M. and Bischof, H.},
	month = sep,
	year = {2009},
	keywords = {Application software, Bagging, Computational efficiency, Computer vision, decision trees, image segmentation, interactive real-time segmentation, learning (artificial intelligence), Machine learning, Machine learning algorithms, online decision tree, online random forest algorithm, Radio frequency, Training data, Usability},
	pages = {1393--1400},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\IZMZGQUG\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\B5DRCZ4P\\Saffari et al. - 2009 - On-line Random Forests.pdf:application/pdf}
}

@article{gast_very_2013,
	title = {Very large scale nearest neighbor search: ideas, strategies and challenges},
	volume = {2},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Very large scale nearest neighbor search},
	url = {http://link.springer.com/article/10.1007/s13735-013-0046-4},
	doi = {10.1007/s13735-013-0046-4},
	abstract = {Web-scale databases and big data collections are computationally challenging to analyze and search. Similarity or more precisely nearest neighbor searches are thus crucial in the analysis, indexing and utilization of these massive multimedia databases. In this work, we begin by reviewing the top approaches from the research literature in the past decade. Furthermore, we evaluate the scalability and computational complexity as the feature complexity and database size vary. For the experiments, we used two different data sets with different dimensionalities. The results reveal interesting insights regarding the index structures and their behavior when the data set size is increased. We also summarized the ideas, strategies and challenges for the future.},
	language = {en},
	number = {4},
	urldate = {2014-11-10},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Gast, Erik and Oerlemans, Ard and Lew, Michael S.},
	month = nov,
	year = {2013},
	keywords = {\$\$k\$\$
                          
                            
                              k
                            
                          
                        -nearest neighbors, big data, Computer Science, general, Data Mining and Knowledge Discovery, High performance indexing, Image Processing and Computer Vision, Information Storage and Retrieval, Information Systems Applications (incl. Internet), Large scale retrieval, Multimedia Information Systems, Similarity search, Web scale search},
	pages = {229--241},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\5XJI7W8N\\Gast et al. - 2013 - Very large scale nearest neighbor search ideas, s.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\7KQ3CG76\\s13735-013-0046-4.html:text/html}
}

@article{friedman_algorithm_1977,
	title = {An Algorithm for Finding Best Matches in Logarithmic Expected Time},
	volume = {3},
	issn = {0098-3500},
	url = {http://doi.acm.org/10.1145/355744.355745},
	doi = {10.1145/355744.355745},
	number = {3},
	urldate = {2014-11-15},
	journal = {{ACM} Trans. Math. Softw.},
	author = {Friedman, Jerome H. and Bentley, Jon Louis and Finkel, Raphael Ari},
	month = sep,
	year = {1977},
	pages = {209--226}
}

@inproceedings{muja_fast_2009,
	title = {Fast approximate nearest neighbors with automatic algorithm configuration},
	abstract = {nearest-neighbors search, randomized kd-trees, hierarchical k-means tree, clustering. For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, “What is the fastest approximate nearest-neighbor algorithm for my data? ” Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection. 1},
	booktitle = {In {VISAPP} International Conference on Computer Vision Theory and Applications},
	author = {Muja, Marius and Lowe, David G.},
	year = {2009},
	pages = {331--340},
	file = {Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\595HRG6B\\summary.html:text/html;Muja_Lowe_2009_Fast approximate nearest neighbors with automatic algorithm configuration.pdf:E\:\\Papers\\Zotero Repository\\storage\\FHHM4PGV\\Muja_Lowe_2009_Fast approximate nearest neighbors with automatic algorithm configuration.pdf:application/pdf}
}

@article{bentley_multidimensional_1975,
	title = {Multidimensional Binary Search Trees Used for Associative Searching},
	volume = {18},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/361002.361007},
	doi = {10.1145/361002.361007},
	abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.},
	number = {9},
	urldate = {2014-11-15},
	journal = {Commun. {ACM}},
	author = {Bentley, Jon Louis},
	month = sep,
	year = {1975},
	keywords = {associative retrieval, attribute, binary search trees, binary tree insertion, information retrieval system, intersection queries, key, nearest neighbor queries, partial match queries},
	pages = {509--517},
	file = {Bentley - 1975 - Multidimensional Binary Search Trees Used for Asso.pdf:E\:\\Papers\\Zotero Repository\\storage\\NWA2T97F\\Bentley - 1975 - Multidimensional Binary Search Trees Used for Asso.pdf:application/pdf}
}

@article{sproull_refinements_1991,
	title = {Refinements to nearest-neighbor searching in k-dimensional trees},
	volume = {6},
	issn = {0178-4617, 1432-0541},
	url = {http://link.springer.com/article/10.1007/BF01759061},
	doi = {10.1007/BF01759061},
	abstract = {This note presents a simplification and generalization of an algorithm for searchingk-dimensional trees for nearest neighbors reported by Friedmanet al [3]. If the distance between records is measured {usingL} 2 , the Euclidean norm, the data structure used by the algorithm to determine the bounds of the search space can be simplified to a single number. Moreover, because distance measurements {inL} 2 are rotationally invariant, the algorithm can be generalized to allow a partition plane to have an arbitrary orientation, rather than insisting that it be perpendicular to a coordinate axis, as in the original algorithm. When ak-dimensional tree is built, this plane can be found from the principal eigenvector of the covariance matrix of the records to be partitioned. These techniques and others yield variants ofk-dimensional trees customized for specific applications. It is wrong to assume thatk-dimensional trees guarantee that a nearest-neighbor query completes in logarithmic expected time. For smallk, logarithmic behavior is observed on all but tiny trees. However, for largerk, logarithmic behavior is achievable only with extremely large numbers of records. Fork = 16, a search of ak-dimensional tree of 76,000 records examines almost every record.},
	language = {en},
	number = {1-6},
	urldate = {2014-11-15},
	journal = {Algorithmica},
	author = {Sproull, Robert F.},
	month = jun,
	year = {1991},
	keywords = {Algorithm Analysis and Problem Complexity, Algorithms, Computer Systems Organization and Communication Networks, Data Structures, Cryptology and Information Theory, k-dimensional tree, Mathematics of Computing, nearest-neighbor search, Searching, Theory of Computation},
	pages = {579--589},
	file = {Snapshot:E\:\\Papers\\Zotero Repository\\storage\\DGHAWQ4P\\BF01759061.html:text/html}
}

@article{arya_optimal_1998,
	title = {An Optimal Algorithm for Approximate Nearest Neighbor Searching Fixed Dimensions},
	volume = {45},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/293347.293348},
	doi = {10.1145/293347.293348},
	abstract = {Consider a set of S of n data points  in real d-dimensional space, Rd, where distances are measured using any Minkowski metric. In nearest neighbor searching, we preprocess S into a data structure, so that given any query point q ∈ Rd, is the closest point of S to q can be reported quickly. Given any positive real \&egr;, data point p is a (1 +\&egr;)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + \&egr;) of the distance to the true nearest neighbor. We show that it is possible to preprocess a    set of n points in     Rd in O(dn log n) time and O(dn) space, so that given a query point  q  ∈ Rd, and \&egr; {\textgreater} 0, a (1 + \&egr;)-approximate nearest neighbor of q can be computed in O(cd, \&egr; log n) time, where cd,\&egr;≤d  1 + 6d/e;d is a factor depending only on dimension and \&egr;. In general, we show that given an integer k ≥ 1, (1 + \&egr;)-approximations  to the  k nearest neighbors of q can  be computed in additional O(kd log n) time.},
	number = {6},
	urldate = {2014-11-15},
	journal = {J. {ACM}},
	author = {Arya, Sunil and Mount, David M. and Netanyahu, Nathan S. and Silverman, Ruth and Wu, Angela Y.},
	month = nov,
	year = {1998},
	keywords = {Approximation algorithms, box-decomposition trees, closet-point queries, nearest neighbor searching, post-office problem, priority search},
	pages = {891--923}
}

@inproceedings{jia_optimizing_2010,
	title = {Optimizing kd-trees for scalable visual descriptor indexing},
	doi = {10.1109/CVPR.2010.5540006},
	abstract = {In this paper, we attempt to scale up the kd-tree indexing methods for large-scale vision applications, e.g., indexing a large number of {SIFT} features and other types of visual descriptors. To this end, we propose an effective approach to generate near-optimal binary space partitioning and need low time cost to access the nodes in the query stage. First, we relax the coordinate-axis-alignment constraint in partition axis selection used in conventional kd-trees, and form a partition axis with the great variance by combining a few coordinate axes in a binary manner for each node, which yields better space partitioning and requires almost the same time cost to visit internal nodes during the query stage thanks to cheap projection operations. Then, we introduce a simple but very effective scheme to guarantee the partition axis of each internal node is orthogonal to or parallel with those of its ancestors, which leads to efficient distance computation between a query point and the cell associated with each node and yields fast priority search. Compared with the conventional kd-trees, our approach takes a little more tree construction time, but obtains much better nearest neighbor search performance. Experimental results on large scale local patch indexing and image search with tiny images show that our approach outperforms the state-of-the-art kd-tree based indexing methods.},
	booktitle = {2010 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Jia, You and Wang, Jingdong and Zeng, Gang and Zha, Hongbin and Hua, Xian-Sheng},
	month = jun,
	year = {2010},
	keywords = {Application software, Asia, binary space partitioning, Computer vision, Costs, distance computation, Image databases, indexing, kd-tree indexing method, Large-scale systems, local patch indexing, Nearest neighbor searches, Neural networks, {SIFT} feature, Spatial databases, visual descriptor indexing, vocabulary},
	pages = {3392--3399},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\QQZ73PWT\\login.html:text/html}
}

@inproceedings{silpa-anan_optimised_2008,
	title = {Optimised {KD}-trees for fast image descriptor matching},
	doi = {10.1109/CVPR.2008.4587638},
	abstract = {In this paper, we look at improving the {KD}-tree for a specific usage: indexing a large number of {SIFT} and other types of image descriptors. We have extended priority search, to priority search among multiple trees. By creating multiple {KD}-trees from the same data set and simultaneously searching among these trees, we have improved the {KD}-treepsilas search performance significantly.We have also exploited the structure in {SIFT} descriptors (or structure in any data set) to reduce the time spent in backtracking. By using Principal Component Analysis to align the principal axes of the data with the coordinate axes, we have further increased the {KD}-treepsilas search performance.},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, 2008. {CVPR} 2008},
	author = {Silpa-Anan, C. and Hartley, R.},
	month = jun,
	year = {2008},
	keywords = {Application software, binary search trees, Binary trees, Computer vision, Image databases, image descriptor matching, image matching, Image recognition, Image retrieval, indexing, multiple {KD}-trees, principal component analysis, Search methods, {SIFT} descriptors, trees (mathematics)},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\IW3Z9XGD\\login.html:text/html}
}

@inproceedings{dasgupta_random_2008,
	address = {New York, {NY}, {USA}},
	series = {{STOC} '08},
	title = {Random Projection Trees and Low Dimensional Manifolds},
	isbn = {978-1-60558-047-0},
	url = {http://doi.acm.org/10.1145/1374376.1374452},
	doi = {10.1145/1374376.1374452},
	abstract = {We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure.},
	urldate = {2014-11-15},
	booktitle = {Proceedings of the Fortieth Annual {ACM} Symposium on Theory of Computing},
	publisher = {{ACM}},
	author = {Dasgupta, Sanjoy and Freund, Yoav},
	year = {2008},
	keywords = {curse of dimension, k-d tree, manifold, random projection},
	pages = {537--546}
}

@article{fukunaga_branch_1975,
	title = {A Branch and Bound Algorithm for Computing k-Nearest Neighbors},
	volume = {C-24},
	issn = {0018-9340},
	doi = {10.1109/T-C.1975.224297},
	abstract = {Computation of the k-nearest neighbors generally requires a large number of expensive distance computations. The method of branch and bound is implemented in the present algorithm to facilitate rapid calculation of the k-nearest neighbors, by eliminating the necesssity of calculating many distances. Experimental results demonstrate the efficiency of the algorithm. Typically, an average of only 61 distance computations were made to find the nearest neighbor of a test sample among 1000 design samples.},
	number = {7},
	journal = {{IEEE} Transactions on Computers},
	author = {Fukunaga, K. and Narendra, Patrenahalli M.},
	month = jul,
	year = {1975},
	keywords = {Branch and bound, distance computation, hierarchical decomposition, k-nearest neighbors, tree-search algorithm.},
	pages = {750--753},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\5PJI4JEX\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\VKFHKX73\\Fukunaga 그리고 Narendra - 1975 - A Branch and Bound Algorithm for Computing k-Neare.pdf:application/pdf}
}

@inproceedings{brin_near_1995,
	address = {Zurich, Switzerland},
	title = {Near Neighbor Search in Large Metric Spaces},
	url = {http://ilpubs.stanford.edu:8090/113/},
	urldate = {2014-11-17},
	author = {Brin, S.},
	year = {1995},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\X876AVX7\\Brin - 1995 - Near Neighbor Search in Large Metric Spaces.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\74CPSZSX\\113.html:text/html}
}

@inproceedings{moore_anchors_2000,
	address = {San Francisco, {CA}, {USA}},
	series = {{UAI}'00},
	title = {The Anchors Hierarchy: Using the Triangle Inequality to Survive High Dimensional Data},
	isbn = {1-55860-709-9},
	shorttitle = {The Anchors Hierarchy},
	url = {http://dl.acm.org/citation.cfm?id=2073946.2073993},
	abstract = {This paper is about metric data structures in high-dimensional or non-Euclidean space that permit cached sufficient statistics accelerations of learning algorithms. It has recently been shown that for less than about 10 dimensions, decorating kd-trees with additional "cached sufficient statistics" such as first and second moments and contingency tables can provide satisfying acceleration for a very wide range of statistical learning tasks such as kernel regression, locally weighted regression, k-means clustering, mixture modeling and Bayes Net learning. In this paper, we begin by defining the anchors hierarchy--a fast data structure and algorithm for localizing data based only on a triangle-inequality-obeying distance metric. We show how this, in its own right, gives a fast and effective clustering of data. But more importantly we show how it can produce a well-balanced structure similar to a Ball-Tree (Omohundro, 1991) or a kind of metric tree (Uhlmann, 1991; Ciaccia, Patella, \& Zezula, 1997) in a way that is neither "topdown" nor "bottom-up" but instead "middleout". We then show how this structure, decorated with cached sufficient statistics, allows a wide variety of statistical learning algorithms to be accelerated even in thousands of dimensions.},
	urldate = {2014-11-17},
	booktitle = {Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Moore, Andrew W.},
	year = {2000},
	pages = {397--405},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\BJW2A3CZ\\Moore - 2000 - The Anchors Hierarchy Using the Triangle Inequali.pdf:application/pdf}
}

@inproceedings{liu_investigation_2004,
	title = {An investigation of practical approximate nearest neighbor algorithms},
	abstract = {This paper concerns approximate nearest neighbor searching algorithms, which have become increasingly important, especially in high dimensional perception areas such as computer vision, with dozens of publications in recent years. Much of this enthusiasm is due to a successful new approximate nearest neighbor approach called Locality Sensitive Hashing ({LSH}). In this paper we ask the question: can earlier spatial data structure approaches to exact nearest neighbor, such as metric trees, be altered to provide approximate answers to proximity queries and if so, how? We introduce a new kind of metric tree that allows overlap: certain datapoints may appear in both the children of a parent. We also introduce new approximate k-{NN} search algorithms on this structure. We show why these structures should be able to exploit the same randomprojection-based approximations that {LSH} enjoys, but with a simpler algorithm and perhaps with greater efficiency. We then provide a detailed empirical evaluation on five large, high dimensional datasets which show up to 31-fold accelerations over {LSH}. This result holds true throughout the spectrum of approximation levels.},
	publisher = {{MIT} Press},
	author = {Liu, Ting and Moore, Andrew W. and Gray, Alexander and Yang, Ke},
	year = {2004},
	pages = {825--832},
	file = {Citeseer - Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\XCIMMAFZ\\Liu et al. - 2004 - An investigation of practical approximate nearest .pdf:application/pdf;Citeseer - Snapshot:E\:\\Papers\\Zotero Repository\\storage\\69AMS7WI\\summary.html:text/html}
}

@article{jegou_product_2011,
	title = {Product Quantization for Nearest Neighbor Search},
	volume = {33},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2010.57},
	abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for {SIFT} and {GIST} image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
	number = {1},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Jegou, H. and Douze, M. and Schmid, C.},
	month = jan,
	year = {2011},
	keywords = {Algorithms, approximate nearest neighbor search, approximate search., Artificial Intelligence, Cartesian product, Cluster Analysis, Electronic mail, Euclidean distance, file organisation, File systems, {GIST} image descriptor, High-dimensional indexing, Image databases, image indexing, Image Interpretation, Computer-Assisted, Image Processing, Computer-Assisted, Image retrieval, indexing, Information Storage and Retrieval, inverted file system, low-dimensional subspace, Models, Statistical, Nearest neighbor searches, Neural networks, Pattern Recognition, Automated, Permission, product quantization, Quantization, Scalability, {SIFT} image descriptor, subspace quantization index, vector quantisation, very large database, very large databases},
	pages = {117--128},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\AQ7MKTAN\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\D3EH9A4B\\Jegou et al. - 2011 - Product Quantization for Nearest Neighbor Search.pdf:application/pdf}
}

@inproceedings{schindler_city-scale_2007,
	title = {City-Scale Location Recognition},
	doi = {10.1109/CVPR.2007.383150},
	abstract = {We look at the problem of location recognition in a large image dataset using a vocabulary tree. This entails finding the location of a query image in a large dataset containing 3times104 streetside images of a city. We investigate how the traditional invariant feature matching approach falls down as the size of the database grows. In particular we show that by carefully selecting the vocabulary using the most informative features, retrieval performance is significantly improved, allowing us to increase the number of database images by a factor of 10. We also introduce a generalization of the traditional vocabulary tree search algorithm which improves performance by effectively increasing the branching factor of a fixed vocabulary tree.},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, 2007. {CVPR} '07},
	author = {Schindler, G. and Brown, M. and Szeliski, R.},
	month = jun,
	year = {2007},
	keywords = {Buildings, Cities and towns, city-scale location recognition, feature matching, Image databases, image matching, Image recognition, Image retrieval, Image storage, Information retrieval, object recognition, query image, Spatial databases, trees (mathematics), visual databases, vocabulary, vocabulary tree},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\GZPGRMKD\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\BKI5EGB9\\Schindler et al. - 2007 - City-Scale Location Recognition.pdf:application/pdf}
}

@inproceedings{taylor_robust_2009-1,
	title = {Robust feature matching in 2.3 \#x00B5;s},
	doi = {10.1109/CVPRW.2009.5204314},
	abstract = {In this paper we present a robust feature matching scheme in which features can be matched in 2.3 mus. For a typical task involving 150 features per image, this results in a processing time of 500 mus for feature extraction and matching. In order to achieve very fast matching we use simple features based on histograms of pixel intensities and an indexing scheme based on their joint distribution. The features are stored with a novel bit mask representation which requires only 44 bytes of memory per feature and allows computation of a dissimilarity score in 20 ns. A training phase gives the patch-based features invariance to small viewpoint variations. Larger viewpoint variations are handled by training entirely independent sets of features from different viewpoints. A complete system is presented where a database of around 13,000 features is used to robustly localise a single planar target in just over a millisecond, including all steps from feature detection to model fitting. The resulting system shows comparable robustness to {SIFT} and Ferns while using a tiny fraction of the processing time, and in the latter case a fraction of the memory as well.},
	booktitle = {{IEEE} Computer Society Conference on Computer Vision and Pattern Recognition Workshops, 2009. {CVPR} Workshops 2009},
	author = {Taylor, S. and Rosten, Edward and Drummond, Tom},
	month = jun,
	year = {2009},
	keywords = {bit mask representation, Computer vision, database indexing, Data mining, feature detection, feature extraction, Histograms, Image databases, Image edge detection, image matching, image representation, indexing, indexing scheme, model fitting, robust feature matching scheme, Robustness, Runtime, Spatial databases, visual databases},
	pages = {15--22},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\QRUHVZT5\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\C933TFGX\\Taylor et al. - 2009 - Robust feature matching in 2.3 #x00B5\;s.pdf:application/pdf}
}

@article{salakhutdinov_semantic_2009,
	title = {Semantic hashing},
	volume = {50},
	issn = {0888-613X},
	url = {http://cat.inist.fr/?aModele=afficheN&cpsidt=21738718},
	language = {eng},
	number = {7},
	urldate = {2014-11-17},
	journal = {International journal of approximate reasoning},
	author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
	year = {2009},
	pages = {969--978},
	file = {Salakhutdinov 그리고 Hinton - 2009 - Semantic hashing.pdf:E\:\\Papers\\Zotero Repository\\storage\\4ER6R9SQ\\Salakhutdinov 그리고 Hinton - 2009 - Semantic hashing.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\22XTRKNH\\cat.inist.fr.html:text/html}
}

@inproceedings{torralba_small_2008,
	title = {Small codes and large image databases for recognition},
	doi = {10.1109/CVPR.2008.4587633},
	abstract = {The Internet contains billions of images, freely available online. Methods for efficiently searching this incredibly rich resource are vital for a large number of applications. These include object recognition, computer graphics, personal photo collections, online image search tools. In this paper, our goal is to develop efficient image search and scene matching techniques that are not only fast, but also require very little memory, enabling their use on standard hardware or even on handheld devices. Our approach uses recently developed machine learning techniques to convert the Gist descriptor (a real valued vector that describes orientation energies at different scales and orientations within an image) to a compact binary code, with a few hundred bits per image. Using our scheme, it is possible to perform real-time searches with millions from the Internet using a single large {PC} and obtain recognition results comparable to the full descriptor. Using our codes on high quality labeled images from the {LabelMe} database gives surprisingly powerful recognition results using simple nearest neighbor techniques.},
	booktitle = {{IEEE} Conference on Computer Vision and Pattern Recognition, 2008. {CVPR} 2008},
	author = {Torralba, A. and Fergus, R. and Weiss, Y.},
	month = jun,
	year = {2008},
	keywords = {Application software, binary codes, compact binary code, Computer graphics, Gist descriptor, Handheld computers, Hardware, image coding, Image databases, image matching, Image recognition, Image retrieval, image search technique, Internet, large image database, Layout, learning (artificial intelligence), Machine learning, object recognition, scene matching technique, search engines, Standards development, very large databases, visual databases},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\TIPMHPM3\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\QGU4UN97\\Torralba et al. - 2008 - Small codes and large image databases for recognit.pdf:application/pdf}
}

@incollection{weiss_spectral_2009,
	title = {Spectral Hashing},
	url = {http://papers.nips.cc/paper/3383-spectral-hashing.pdf},
	urldate = {2014-11-17},
	booktitle = {Advances in Neural Information Processing Systems 21},
	publisher = {Curran Associates, Inc.},
	author = {Weiss, Yair and Torralba, Antonio and Fergus, Rob},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	year = {2009},
	pages = {1753--1760},
	file = {NIPS Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\ZQHD9T2K\\Weiss et al. - 2009 - Spectral Hashing.pdf:application/pdf;NIPS Snapshort:E\:\\Papers\\Zotero Repository\\storage\\P3M3W84B\\3383-spectral-hashing.html:text/html}
}

@inproceedings{gionis_similarity_1999-1,
	address = {San Francisco, {CA}, {USA}},
	series = {{VLDB} '99},
	title = {Similarity Search in High Dimensions via Hashing},
	isbn = {1-55860-615-7},
	url = {http://dl.acm.org/citation.cfm?id=645925.671516},
	urldate = {2014-11-17},
	booktitle = {Proceedings of the 25th International Conference on Very Large Data Bases},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
	year = {1999},
	pages = {518--529}
}

@inproceedings{lv_multi-probe_2007,
	address = {Vienna, Austria},
	series = {{VLDB} '07},
	title = {Multi-probe {LSH}: Efficient Indexing for High-dimensional Similarity Search},
	isbn = {978-1-59593-649-3},
	shorttitle = {Multi-probe {LSH}},
	url = {http://dl.acm.org/citation.cfm?id=1325851.1325958},
	abstract = {Similarity indices for high-dimensional data are very desirable for building content-based search systems for feature-rich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing ({LSH}) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe {LSH} that overcomes this drawback. Multi-probe {LSH} is built on the well-known {LSH} technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based {LSH} designed to reduce the space requirement of the basic {LSH} method. We have implemented the multi-probe {LSH} method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe {LSH} method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe {LSH} has a similar time-efficiency as the basic {LSH} method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based {LSH} method, to achieve the same search quality, multi-probe {LSH} uses less query time and 5 to 8 times fewer number of hash tables.},
	urldate = {2014-11-17},
	booktitle = {Proceedings of the 33rd International Conference on Very Large Data Bases},
	publisher = {{VLDB} Endowment},
	author = {Lv, Qin and Josephson, William and Wang, Zhe and Charikar, Moses and Li, Kai},
	year = {2007},
	pages = {950--961},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\GFJMIB58\\Lv et al. - 2007 - Multi-probe LSH Efficient Indexing for High-dimen.pdf:application/pdf}
}

@inproceedings{bawa_lsh_2005,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '05},
	title = {{LSH} Forest: Self-tuning Indexes for Similarity Search},
	isbn = {1-59593-046-9},
	shorttitle = {{LSH} Forest},
	url = {http://doi.acm.org/10.1145/1060745.1060840},
	doi = {10.1145/1060745.1060840},
	abstract = {We consider the problem of indexing high-dimensional data for answering (approximate) similarity-search queries. Similarity indexes prove to be important in a wide variety of settings: Web search engines desire fast, parallel, main-memory-based indexes for similarity search on text data; database systems desire disk-based similarity indexes for high-dimensional data, including text and images; peer-to-peer systems desire distributed similarity indexes with low communication cost. We propose an indexing scheme called {LSH} Forest which is applicable in all the above contexts. Our index uses the well-known technique of locality-sensitive hashing ({LSH}), but improves upon previous designs by (a) eliminating the different data-dependent parameters for which {LSH} must be constantly hand-tuned, and (b) improving on {LSH}'s performance guarantees for skewed data distributions while retaining the same storage and query overhead. We show how to construct this index in main memory, on disk, in parallel systems, and in peer-to-peer systems. We evaluate the design with experiments on multiple text corpora and demonstrate both the self-tuning nature and the superior performance of {LSH} Forest.},
	urldate = {2014-11-17},
	booktitle = {Proceedings of the 14th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Bawa, Mayank and Condie, Tyson and Ganesan, Prasanna},
	year = {2005},
	keywords = {peer-to-peer (P2P), similarity indexes},
	pages = {651--660},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\9R3SCEG5\\Bawa et al. - 2005 - LSH Forest Self-tuning Indexes for Similarity Sea.pdf:application/pdf}
}

@techreport{cannons_review_2008,
	address = {York University},
	type = {Technical Report},
	title = {A Review of Visual Tracking},
	abstract = {This report contains a review of visual tracking in monocular video sequences. For the purpose of this review, the majority of the visual trackers in the literature are divided into three tracking categories: discrete feature trackers, contour trackers, and region-based trackers. This categorization was performed based on the features used and the algorithms employed by the various visual trackers. The first class of trackers represents targets as discrete features (e.g. points, sets of points, lines) and performs data association using a distance metric that accommodates the particular feature. Contour trackers provide precise outlines of the target boundaries, meaning that they must not only uncover the position of the target, but its shape as well. Contour trackers often make use of gradient edge information during the tracking process. Region trackers represent the target with area-based descriptors that define its support and attempt to locate the image region in the current frame that best matches an object template. Trackers that are not in agreement with the abovementioned categorization, including those that combine methods from the three defined classes, are also considered in this review. In addition to categorizing and describing the various visual trackers in the literature, this review also provides a commentary on the current state of the field as well as a comparative analysis of the various approaches. The paper concludes with an outline of open problems in visual tracking.},
	number = {{CSE}-2008-07},
	author = {Cannons, Kevin},
	month = sep,
	year = {2008},
	file = {Cannons - 2008 - A Review of Visual Tracking.pdf:E\:\\Papers\\Zotero Repository\\storage\\9BJTN2DQ\\Cannons - 2008 - A Review of Visual Tracking.pdf:application/pdf}
}

@incollection{iglesias_evaluation_2014,
	series = {Lecture Notes in Computer Science},
	title = {Evaluation of Keypoint Descriptors for Gender Recognition},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-12567-1, 978-3-319-12568-8},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-12568-8_69},
	abstract = {Gender recognition is a relevant problem due to the number and importance of its possible application areas. The challenge is to achieve high recognition rates in the shortest possible time. Most studies are based on Local Binary Patterns ({LBP}) and its variants to estimate gender. In this paper, we propose the use of Binary Robust Independent Elementary Features ({BRIEF}), Oriented {FAST} and Rotated {BRIEF} ({ORB}) and Binary Robust Invariant Scalable Keypoints ({BRISK}) in gender recognition due to their good performance and speed. The aim is to show that {ORB} and {BRISK} are faster than {LBP} but allow to achieve similar recognition rates, which makes them suitable for real-time systems. For the best of our knowledge, it has not been studied in literature.},
	language = {en},
	number = {8827},
	urldate = {2014-11-10},
	booktitle = {Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications},
	publisher = {Springer International Publishing},
	author = {Iglesias, Florencia Soledad and Buemi, María Elena and Acevedo, Daniel and Jacobo-Berlles, Julio},
	editor = {Bayro-Corrochano, Eduardo and Hancock, Edwin},
	month = jan,
	year = {2014},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Biometrics, Gender recognition, Image Processing and Computer Vision, Information Systems Applications (incl. Internet), Keypoint Descriptors, {LBP}, Pattern Recognition},
	pages = {564--571},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\5UICNMQG\\Iglesias et al. - 2014 - Evaluation of Keypoint Descriptors for Gender Reco.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\QJA79CHI\\978-3-319-12568-8_69.html:text/html}
}

@incollection{niedermayer_retrieval_2014,
	series = {Lecture Notes in Computer Science},
	title = {Retrieval of Binary Features in Image Databases: A Study},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-11987-8, 978-3-319-11988-5},
	shorttitle = {Retrieval of Binary Features in Image Databases},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-11988-5_14},
	abstract = {Many state-of-the art object recognition systems rely on local image features, sometimes hundreds per image, that describe the surroundings of detected interest points by a high-dimensional feature vector. To recognize objects, these systems have to match features detected in a query image against the features stored in a database containing millions or even billions of feature vectors. Hence, efficient matching is crucial for real applications. In the past, feature vectors were often real-valued, and therefore research focused on such feature representations. Present techniques, however, involve binary features to reduce memory consumption and to speed up the feature extraction stage. Matching such binary features received relatively little attention in the computer vision community. Often, either Locality Sensitive Hashing ({LSH}) or quantization-based techniques, that are known from real-valued features, are used. However, an in-depth evaluation of the involved parameters in binary space has, to the best of our knowledge, not yet been performed. In this paper, we aim at closing this research gap, providing valuable insights for application-oriented research.},
	language = {en},
	number = {8821},
	urldate = {2014-11-10},
	booktitle = {Similarity Search and Applications},
	publisher = {Springer International Publishing},
	author = {Niedermayer, Johannes and Kröger, Peer},
	editor = {Traina, Agma Juci Machado and Jr, Caetano Traina and Cordeiro, Robson Leonardo Ferreira},
	month = jan,
	year = {2014},
	keywords = {Algorithm Analysis and Problem Complexity, Database Management, Data structures, Information Storage and Retrieval, Numeric Computing, Pattern Recognition},
	pages = {151--163},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\IM2EKKVM\\Niedermayer 그리고 Kröger - 2014 - Retrieval of Binary Features in Image Databases A.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\DDICGI45\\978-3-319-11988-5_14.html:text/html}
}

@incollection{ma_fast_2014,
	series = {Lecture Notes in Computer Science},
	title = {Fast Search of Binary Codes with Distinctive Bits},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-13167-2, 978-3-319-13168-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-13168-9_31},
	abstract = {Although distance between binary codes can be computed fast in hamming space, linear search is not practical for large scale dataset. Therefore attention has been paid to the efficiency of performing approximate nearest neighbor search, in which Hierarchical Clustering Trees ({HCT}) is the state-of-the-art method. However, {HCT} builds index with the whole binary codes, which degrades search performance. In this paper, we first propose an algorithm to compress binary codes by extracting distinctive bits according to the standard deviation of each bit. Then, a new index is proposed using com-pressed binary codes based on hierarchical decomposition of binary spaces. Experiments conducted on reference datasets and a dataset of one billion binary codes demonstrate the effectiveness and efficiency of our method.},
	language = {en},
	number = {8879},
	urldate = {2014-11-10},
	booktitle = {Advances in Multimedia Information Processing – {PCM} 2014},
	publisher = {Springer International Publishing},
	author = {Ma, Yanping and Xie, Hongtao and Chen, Zhineng and Dai, Qiong and Huang, Yinfei and Ji, Guangrong},
	editor = {Ooi, Wei Tsang and Snoek, Cees G. M. and Tan, Hung Khoon and Ho, Chin-Kuan and Huet, Benoit and Ngo, Chong-Wah},
	month = jan,
	year = {2014},
	keywords = {approximate nearest neighbor search, binary codes, binary indexing, Data Mining and Knowledge Discovery, Image Processing and Computer Vision, Information Systems Applications (incl. Internet), Multimedia Information Systems, Pattern Recognition, User Interfaces and Human Computer Interaction},
	pages = {274--283},
	file = {Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\HIUZWEBG\\Ma et al. - 2014 - Fast Search of Binary Codes with Distinctive Bits.pdf:application/pdf;Snapshot:E\:\\Papers\\Zotero Repository\\storage\\KGN3B4JU\\978-3-319-13168-9_31.html:text/html}
}

@inproceedings{wagner_pose_2008-1,
	address = {Washington, {DC}, {USA}},
	series = {{ISMAR} '08},
	title = {Pose Tracking from Natural Features on Mobile Phones},
	isbn = {978-1-4244-2840-3},
	url = {http://dx.doi.org/10.1109/ISMAR.2008.4637338},
	doi = {10.1109/ISMAR.2008.4637338},
	abstract = {In this paper we present two techniques for natural feature tracking in real-time on mobile phones. We achieve interactive frame rates of up to 20Hz for natural feature tracking from textured planar targets on current-generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely {SIFT} and Ferns. While {SIFT} is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. We present evaluations on robustness and performance on various devices and finally discuss their appropriateness for Augmented Reality applications.},
	urldate = {2014-05-14},
	booktitle = {Proceedings of the 7th {IEEE}/{ACM} International Symposium on Mixed and Augmented Reality},
	publisher = {{IEEE} Computer Society},
	author = {Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, Dieter},
	year = {2008},
	pages = {125--134},
	file = {ACM Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\Q3JTBN9E\\Wagner et al. - 2008 - Pose Tracking from Natural Features on Mobile Phon.pdf:application/pdf}
}

@inproceedings{wagner_multiple_2009-1,
	title = {Multiple target detection and tracking with guaranteed framerates on mobile phones},
	doi = {10.1109/ISMAR.2009.5336497},
	abstract = {In this paper we present a novel method for real-time pose estimation and tracking on low-end devices such as mobile phones. The presented system can track multiple known targets in real-time and simultaneously detect new targets for tracking. We present a method to automatically and dynamically balance the quality of detection and tracking to adapt to a variable time budget and ensure a constant frame rate. Results from real data of a mobile phone Augmented Reality system demonstrate the efficiency and robustness of the described approach. The system can track 6 planar targets on a mobile phone simultaneously at framerates of 23 fps.},
	booktitle = {8th {IEEE} International Symposium on Mixed and Augmented Reality, 2009. {ISMAR} 2009},
	author = {Wagner, Daniel and Schmalstieg, D. and Bischof, H.},
	month = oct,
	year = {2009},
	keywords = {6DOF, augmented reality, augmented reality system, Cameras, constant frame rate, mobile computing, mobile handsets, mobile phone, multiple target detection, natural features, object detection, Optical filters, Optical mixing, Optical sensors, pose estimation, real-time pose estimation, Real time systems, Robustness, target tracking, tracking},
	pages = {57--64},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\TZRUJIUQ\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\H8V6D4C7\\Wagner et al. - 2009 - Multiple target detection and tracking with guaran.pdf:application/pdf}
}

@article{wagner_real-time_2010-1,
	title = {Real-Time Detection and Tracking for Augmented Reality on Mobile Phones},
	volume = {16},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2009.99},
	abstract = {In this paper, we present three techniques for 6DOF natural feature tracking in real time on mobile phones. We achieve interactive frame rates of up to 30 Hz for natural feature tracking from textured planar targets on current generation phones. We use an approach based on heavily modified state-of-the-art feature descriptors, namely {SIFT} and Ferns plus a template-matching-based tracker. While {SIFT} is known to be a strong, but computationally expensive feature descriptor, Ferns classification is fast, but requires large amounts of memory. This renders both original designs unsuitable for mobile phones. We give detailed descriptions on how we modified both approaches to make them suitable for mobile phones. The template-based tracker further increases the performance and robustness of the {SIFT}- and Ferns-based approaches. We present evaluations on robustness and performance and discuss their appropriateness for Augmented Reality applications.},
	number = {3},
	journal = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Wagner, Daniel and Reitmayr, Gerhard and Mulloni, Alessandro and Drummond, Tom and Schmalstieg, D.},
	month = may,
	year = {2010},
	keywords = {Algorithms, and virtual realities, artificial, augmented, augmented reality, Cellular Phone, Computer graphics, Computer Systems, feature descriptor, feature tracking, Image Interpretation, Computer-Assisted, Image Processing and Computer Vision, Imaging, Three-Dimensional, Information interfaces and presentation, Information Storage and Retrieval, interactive frame rates, mobile computing, mobile handsets, mobile phones, Multimedia Information Systems, multimedia systems, Pattern Recognition, Automated, real-time detection, real-time systems, real-time tracking, scene analysis, Software, state-of-the-art feature descriptors, template-matching-based tracker, tracking., User-Computer Interface, vocabulary},
	pages = {355--368},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\4GFX3JKF\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\PG7QI7JQ\\Wagner et al. - 2010 - Real-Time Detection and Tracking for Augmented Rea.pdf:application/pdf}
}

@inproceedings{wagner_robust_2008,
	title = {Robust and unobtrusive marker tracking on mobile phones},
	doi = {10.1109/ISMAR.2008.4637337},
	abstract = {Marker tracking has revolutionized augmented reality about a decade ago. However, this revolution came at the expense of visual clutter. In this paper, we propose several new marker techniques, which are less obtrusive than the usual black and white squares. Furthermore, we report methods that allow tracking beyond the visibility of these markers further improving robustness. All presented techniques are implemented in a single tracking library, are highly efficient in their memory and {CPU} usage and run at interactive frame rates on mobile phones.},
	booktitle = {7th {IEEE}/{ACM} International Symposium on Mixed and Augmented Reality, 2008. {ISMAR} 2008},
	author = {Wagner, Daniel and Langlotz, Tobias and Schmalstieg, D.},
	month = sep,
	year = {2008},
	keywords = {augmented reality, Central Processing Unit, H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems — Artificial, augmented, and virtual realities, I.4.8 [Image Processing and Computer Vision]: Scene Analysis — Tracking, Information systems, {ISO} standards, Libraries, marker tracking, mobile computing, mobile handsets, mobile phone marker tracking, mobile phones, multimedia systems, Pixel, pixel flow, Robustness, Shape, visual clutter},
	pages = {121--124},
	file = {IEEE Xplore Abstract Record:E\:\\Papers\\Zotero Repository\\storage\\RKQBMZ9J\\abs_all.html:text/html;IEEE Xplore Full Text PDF:E\:\\Papers\\Zotero Repository\\storage\\8BA5NV5X\\Wagner et al. - 2008 - Robust and unobtrusive marker tracking on mobile p.pdf:application/pdf}
}